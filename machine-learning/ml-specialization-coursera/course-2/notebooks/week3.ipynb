{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c98b7b0b-58b1-4503-9f7d-db8f31410179",
   "metadata": {},
   "source": [
    "# Week 3: Advice for applying machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae630642-8877-4dda-9935-e41e4e3915d3",
   "metadata": {},
   "source": [
    "## Deciding what to try next\n",
    "\n",
    "Here we introduce the topic of **Machine Learning Diagnostics**, stressing the importance of making efficient, data-driven decisions during a machine learning project to save significant time and effort.\n",
    "\n",
    "### 1. The Need for Efficiency\n",
    "* **Time Savings:** An efficient team can often complete a machine learning system in a few weeks, while a less skilled team might take six months for the same project.\n",
    "* **Effective Decisions:** The key to efficiency is the ability to repeatedly make **good decisions about what to do next** to improve the model's performance.\n",
    "\n",
    "### 2. Common Improvement Strategies\n",
    "When a machine learning model (e.g., regularized linear regression) shows unacceptable error, there are many common, but not always fruitful, steps one might try:\n",
    "* **Data Quantity:** Get **more training examples**.\n",
    "* **Feature Quantity:** Try a **smaller set of features** (feature selection).\n",
    "* **Feature Quality:** Get **additional features** (feature engineering).\n",
    "* **Feature Transformation:** Add **polynomial features** (e.g., $x_1^2, x_1x_2$).\n",
    "* **Regularization:** **Decrease** the regularization parameter ($\\lambda$).\n",
    "* **Regularization:** **Increase** the regularization parameter ($\\lambda$).\n",
    "\n",
    "### 3. The Role of Diagnostics\n",
    "* **Problem:** Blindly trying the strategies above often wastes time (e.g., spending months collecting data that doesn't ultimately help).\n",
    "* **Solution:** A **diagnostic** is a test you can run to gain insight into **what is or isn't working** with your current learning algorithm.\n",
    "* **Guidance:** Diagnostics provide objective guidance on which investment of time (e.g., which strategy from the list above) is most likely to improve the algorithm's performance.\n",
    "* **Value:** While diagnostics take time to implement, running them is a crucial step that can save months of wasted effort.\n",
    "\n",
    "### 4. Next Step\n",
    "The immediate next step in this study is to learn **how to evaluate the performance** of a learning algorithm, which is necessary before any meaningful diagnostic can be run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cecca94-4eba-46ce-bbfe-9a0c6d7bda55",
   "metadata": {},
   "source": [
    "## Evaluating a model\n",
    "\n",
    "Here we introduce the fundamental process of **evaluating a machine learning model** by splitting the dataset into training and test sets and calculating error metrics.\n",
    "\n",
    "### 1. The Need for Systematic Evaluation\n",
    "* **Problem:** For models with many features (more than 1 or 2), it is impossible to visually plot the function $f(x)$ to determine if the model is a good fit.\n",
    "* **Goal:** A systematic method is required to tell if a model that fits the **training data** well will also **generalize** to new, unseen examples.\n",
    "\n",
    "### 2. Splitting the Dataset\n",
    "* **Method:** The entire dataset is split into two subsets:\n",
    "    * **Training Set ($m_{\\text{train}}$):** Typically **70% to 80%** of the data. Used exclusively to **train** the model (i.e., minimize the cost function $J$).\n",
    "    * **Test Set ($m_{\\text{test}}$):** Typically **20% to 30%** of the data. Used exclusively to **evaluate** the trained model's generalization ability.\n",
    "\n",
    "### 3. Evaluating Performance (Regression - Squared Error Cost)\n",
    "* **Training Cost (Used to fit $W, b$):** The standard cost function **including the regularization term** is minimized to find the parameters:\n",
    "    $$J(W, b) = \\frac{1}{2m_{\\text{train}}} \\sum_{i=1}^{m_{\\text{train}}} (f(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2$$\n",
    "* **Training Error ($J_{\\text{train}}$):** The average squared error calculated **only on the training set** and **without the regularization term**:\n",
    "    $$J_{\\text{train}}(W, b) = \\frac{1}{2m_{\\text{train}}} \\sum_{i=1}^{m_{\\text{train}}} (f(x^{(i)}) - y^{(i)})^2$$\n",
    "* **Test Error ($J_{\\text{test}}$):** The average squared error calculated **only on the test set** and **without the regularization term**:\n",
    "    $$J_{\\text{test}}(W, b) = \\frac{1}{2m_{\\text{test}}} \\sum_{i=1}^{m_{\\text{test}}} (f(x_{\\text{test}}^{(i)}) - y_{\\text{test}}^{(i)})^2$$\n",
    "* **Diagnosis:** A large difference where $J_{\\text{train}}$ is low but $J_{\\text{test}}$ is high indicates a **poorly generalizing model** (overfitting).\n",
    "\n",
    "### 4. Evaluating Performance (Classification)\n",
    "For classification (e.g., Logistic Regression), two common methods are used for evaluation:\n",
    "\n",
    "| Evaluation Metric | Description |\n",
    "| :--- | :--- |\n",
    "| **Average Loss** | The average logistic loss (cross-entropy) calculated on the test set (analogous to $J_{\\text{test}}$ in regression, without regularization). |\n",
    "| **Misclassification Error** (Most Common) | The **fraction of examples** in the test set where the model's prediction ($\\hat{y}$) does not equal the actual label ($y$). |\n",
    "| **Calculation:** | The model predicts $\\hat{y} = 1$ (if $f(x) \\ge 0.5$) or $\\hat{y} = 0$ (if $f(x) < 0.5$). The error is the count of $(\\hat{y} \\ne y)$ divided by $m_{\\text{test}}$. |\n",
    "\n",
    "### 5. Future Steps\n",
    "Splitting the data into training and test sets is the first step toward enabling an algorithm to **automatically choose the best model** (e.g., deciding between a second-order or fourth-order polynomial), which requires one further refinement to the data splitting idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f574f8ba-f93e-46f8-b7f4-53dd92f62ee4",
   "metadata": {},
   "source": [
    "## Model selection and training/cross validation/test sets\n",
    "\n",
    "Here are details the proper procedure for **Model Selection** in machine learning, which involves splitting the data into three subsets to prevent an overly optimistic estimate of generalization error.\n",
    "\n",
    "### 1. The Flaw in Two-Set Evaluation\n",
    "* **Problem:** If you split your data into only a training set and a test set, and then use the test set to **select** the best model (e.g., choosing a 5th-degree polynomial because it has the lowest $J_{\\text{test}}$), the resulting $J_{\\text{test}}$ error will be an **overly optimistic (lower) estimate** of the true generalization error.\n",
    "* **Reason:** You have effectively **fit a hyperparameter** (the polynomial degree, $d$) to the test set, making the test set error an unreliable estimate of performance on truly new data.\n",
    "\n",
    "### 2. The Solution: Three-Way Data Split\n",
    "To enable automatic model selection without corrupting the final performance estimate, the data must be split into three distinct subsets:\n",
    "\n",
    "| Subset | Typical Split | Purpose | Notation |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Training Set** | 60% | Used to **fit (train)** the model's parameters ($W, b$). | $m_{\\text{train}}$ |\n",
    "| **Cross-Validation Set** | 20% | Used to **select** the best model (e.g., choose polynomial degree $d$, or neural network architecture). | $m_{\\text{cv}}$ |\n",
    "| **Test Set** | 20% | Used only **once** at the very end to give an **unbiased final estimate** of the generalization error. | $m_{\\text{test}}$ |\n",
    "\n",
    "* **Terminology:** The Cross-Validation set (CV set) is also commonly called the **Validation Set** or the **Development Set (Dev Set)**.\n",
    "\n",
    "### 3. The Model Selection Procedure\n",
    "The process for automatically choosing the best model (Model Selection) uses all three sets:\n",
    "\n",
    "1.  **Fit Models:** Train all candidate models (e.g., $d=1$ through $d=10$ polynomials, or different neural network architectures) only on the **Training Set**. This yields a set of parameters ($W^1, b^1$), ($W^2, b^2$), etc.\n",
    "2.  **Select Best Model:** Evaluate the performance of all trained models using the **Cross-Validation Error** ($J_{\\text{cv}}$).\n",
    "    * **Pick the model** that yields the lowest $J_{\\text{cv}}$. (e.g., if $J_{\\text{cv}}$ is lowest for the 4th-order polynomial, you select that model).\n",
    "3.  **Final Report:** Report the generalization performance of the **selected model** using its error on the **Test Set** ($J_{\\text{test}}$).\n",
    "\n",
    "### 4. Best Practice\n",
    "* **Rule:** All decisions about the model (fitting parameters $W, b$, and choosing hyperparameters like polynomial degree or architecture) must be made using only the **Training** and **Cross-Validation** sets.\n",
    "* **Integrity:** The **Test Set must be untouched** during the decision-making process. This ensures that the final reported $J_{\\text{test}}$ is a **fair** (not overly optimistic) estimate of how well the chosen model will perform on truly new data in the real world.\n",
    "\n",
    "The next topic, **Bias and Variance**, is introduced as the most powerful diagnostic tool, relying on these three error measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40254454-ad0f-4a8e-8003-108b6e19224b",
   "metadata": {},
   "source": [
    "## Notes from C2W3_Lab_01_Model_Evaluation_and_Selection\n",
    "\n",
    "Scikit-learn also has a built-in `mean_squared_error()` function that you can use. Take note though that [as per the documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error), scikit-learn's implementation only divides by `m` and not `2*m`, where `m` is the number of examples. As mentioned in Course 1 of this Specialization (cost function lectures), dividing by `2m` is a convention we will follow but the calculations should still work whether or not you include it. Thus, to match the equation above, you can use the scikit-learn function then divide by 2 as shown below. We also included a for-loop implementation so you can check that it's equal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356dbcf3-59f0-4b2d-85f2-f49186270e2d",
   "metadata": {},
   "source": [
    "## Diagnosing bias and variance\n",
    "\n",
    "Here we explain the concepts of **Bias** and **Variance** in machine learning, and how to use the **Training Error ($J_{\\text{train}}$)** and **Cross-Validation Error ($J_{\\text{cv}}$)** to diagnose these problems, which then guides the next steps for model improvement.\n",
    "\n",
    "### 1. The Purpose of Bias and Variance Diagnostics\n",
    "* **Goal:** To systematically determine the reason for a model's poor performance to guide the next steps for improvement.\n",
    "* **Method:** Instead of relying on visualizations (which aren't possible with many features), a model's performance on the **training set** and the **cross-validation set** provides key diagnostic insights.\n",
    "\n",
    "### 2. Diagnosing Bias (Underfitting)\n",
    "* **Visual Analogy:** Fitting a straight line to curvilinear data ($d=1$).\n",
    "* **Model Characteristic:** The model is too simple (e.g., too few features or a low-degree polynomial) and cannot capture the complexity of the data.\n",
    "* **Diagnostic Signature:**\n",
    "    * **$J_{\\text{train}}$ is HIGH:** The model performs poorly even on the data it was trained on. This is the **strongest indicator of high bias**.\n",
    "    * **$J_{\\text{cv}}$ is $\\approx$ HIGH:** The cross-validation error is similar to the training error (both are high).\n",
    "\n",
    "### 3. Diagnosing Variance (Overfitting)\n",
    "* **Visual Analogy:** Fitting a very wiggly, high-degree polynomial ($d=4$) to a small dataset.\n",
    "* **Model Characteristic:** The model is too complex and fits the noise and specific examples of the training data too closely, failing to generalize to new data.\n",
    "* **Diagnostic Signature:**\n",
    "    * **$J_{\\text{train}}$ is LOW:** The model performs great on the training set.\n",
    "    * **$J_{\\text{cv}}$ is MUCH GREATER than $J_{\\text{train}}$:** The large gap between the two errors indicates the model performs well on seen data but poorly on unseen data. This is the **strongest indicator of high variance**.\n",
    "\n",
    "### 4. The \"Just Right\" Model\n",
    "* **Diagnostic Signature:**\n",
    "    * **$J_{\\text{train}}$ is LOW** (but not zero).\n",
    "    * **$J_{\\text{cv}}$ is LOW** and is **close to** $J_{\\text{train}}$.\n",
    "\n",
    "### 5. Relationship Between Error and Model Complexity\n",
    "When plotting the error metrics against the model complexity (e.g., degree of polynomial, $d$):\n",
    "* **$J_{\\text{train}}$:** Typically **decreases** as model complexity ($d$) increases, because a more complex model can always fit the training data better.\n",
    "* **$J_{\\text{cv}}$:** Forms a **U-shaped curve**:\n",
    "    * **Low $d$ (High Bias):** $J_{\\text{cv}}$ is high because the model underfits.\n",
    "    * **Optimal $d$:** $J_{\\text{cv}}$ is at its minimum (\"just right\").\n",
    "    * **High $d$ (High Variance):** $J_{\\text{cv}}$ rises again because the model overfits and fails to generalize.\n",
    "\n",
    "![Variance and Bias](images/var_bias.png)\n",
    "\n",
    "### 6. Simultaneous High Bias and High Variance\n",
    "* **Possibility:** While less common in simple linear regression, it is possible in complex models (like neural networks) to simultaneously have both problems.\n",
    "* **Diagnostic Signature:**\n",
    "    * **$J_{\\text{train}}$ is HIGH** (indicating bias/underfitting on the training data).\n",
    "    * **$J_{\\text{cv}}$ is MUCH GREATER than $J_{\\text{train}}$** (indicating variance/poor generalization).\n",
    "\n",
    "**Key Takeaways:**\n",
    "* **High Bias $\\implies$ $J_{\\text{train}}$ is High.**\n",
    "* **High Variance $\\implies$ $J_{\\text{cv}}$ $\\gg J_{\\text{train}}$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0194a0b-5865-47a1-8082-999c3b86a8de",
   "metadata": {},
   "source": [
    "## Regularization and bias/variance\n",
    "\n",
    "Here we touch on the effect of the **regularization parameter ($\\lambda$)** on a model's bias and variance, and demonstrates how to use the **cross-validation set** to select the optimal value for $\\lambda$.\n",
    "\n",
    "### 1. Effect of $\\lambda$ on Bias and Variance\n",
    "\n",
    "The regularization parameter $\\lambda$ controls the trade-off between fitting the training data well and keeping the model weights ($W$) small. This directly impacts bias and variance:\n",
    "\n",
    "| Value of $\\lambda$ | Model Behavior | Diagnostic Signature | Problem |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Very Large** (e.g., 10,000) | Parameters $W$ are forced to near zero, making $f(x) \\approx b$ (a constant line). | **$J_{\\text{train}}$ is HIGH** and $J_{\\text{cv}}$ is HIGH. | **High Bias (Underfitting)** |\n",
    "| **Very Small** (e.g., 0) | No regularization; the model is free to fit the training noise. | $J_{\\text{train}}$ is LOW, but **$J_{\\text{cv}}$ is MUCH GREATER than $J_{\\text{train}}$**. | **High Variance (Overfitting)** |\n",
    "| **Intermediate** | The model is \"just right,\" balancing fit and simplicity. | $J_{\\text{train}}$ is LOW and $J_{\\text{cv}}$ is LOW and close to $J_{\\text{train}}$. | **Good Generalization** |\n",
    "\n",
    "### 2. Using Cross-Validation to Select $\\lambda$\n",
    "\n",
    "The cross-validation set is used to automatically select the best $\\lambda$ value, similar to how it's used to select the best polynomial degree ($D$).\n",
    "\n",
    "1.  **Define a Range:** Choose a wide range of potential $\\lambda$ values to test (e.g., $\\lambda=0, 0.01, 0.02, 0.04, \\dots, 10$).\n",
    "2.  **Train Models:** For each chosen $\\lambda$, train the model (e.g., a 4th-order polynomial) by minimizing the cost function on the **Training Set**. Each $\\lambda$ yields a different set of parameters ($W_i, b_i$).\n",
    "3.  **Evaluate on CV Set:** Calculate the **Cross-Validation Error** ($J_{\\text{cv}}$) for each set of parameters ($W_i, b_i$).\n",
    "4.  **Select Optimal $\\lambda$:** Pick the $\\lambda$ value that resulted in the **lowest $J_{\\text{cv}}$ error**.\n",
    "5.  **Final Report:** Report the model's final generalization performance using the **Test Set Error** ($J_{\\text{test}}$) only on the chosen parameters.\n",
    "\n",
    "### 3. Error vs. $\\lambda$ Plot\n",
    "\n",
    "When plotting the errors as a function of $\\lambda$ (on a logarithmic scale):\n",
    "\n",
    "* **$J_{\\text{train}}$ Curve:** **Increases** as $\\lambda$ increases. A higher $\\lambda$ forces the optimizer to prioritize smaller $W$ values over minimizing training error.\n",
    "* **$J_{\\text{cv}}$ Curve:** Forms a **U-shaped curve**:\n",
    "    * It is high on the left (small $\\lambda$, high variance).\n",
    "    * It is high on the right (large $\\lambda$, high bias).\n",
    "    * It dips to a minimum in the middle, representing the optimal $\\lambda$ value for best generalization.\n",
    "\n",
    "This diagnostic plot reinforces that cross-validation is essential for finding the **sweet spot** where the model avoids both high bias and high variance.\n",
    "\n",
    "![Variance and bias for lambda](images/var_bias_lambda.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2af73a-495b-4dd6-818c-573868579abe",
   "metadata": {},
   "source": [
    "## Stablishing a baseline level of performance\n",
    "\n",
    "This section refines the diagnosis of **Bias and Variance** by introducing the concept of a **Baseline Level of Performance** (often Human-Level Performance). This benchmark allows for a more meaningful judgment of whether the training error is \"high.\"\n",
    "\n",
    "### 1. The Need for a Baseline\n",
    "\n",
    "* **Problem with Absolute Error:** Simply looking at the training error ($J_{\\text{train}}$) in isolation (e.g., concluding 10.8% error is \"high\") can be misleading. For many tasks (especially those with noisy unstructured data like speech or images), achieving 0% error is impossible.\n",
    "* **Baseline Defined:** The **Baseline Level of Performance** is the error rate you can reasonably hope your learning algorithm can eventually reach.\n",
    "    * **Common Baseline:** **Human-Level Performance** (HLP) is a good benchmark for tasks that humans are proficient at (vision, speech, text).\n",
    "    * **Other Baselines:** A previous algorithm's performance, a competitor's result, or a guess based on prior experience.\n",
    "\n",
    "### 2. Refining the Bias-Variance Diagnosis\n",
    "\n",
    "The diagnosis is based on two key gaps, relative to the Baseline:\n",
    "\n",
    "####  Gap 1: Baseline to Training Error ($\\text{Baseline} \\rightarrow J_{\\text{train}}$)\n",
    "This gap indicates the severity of the **High Bias** (Underfitting) problem.\n",
    "\n",
    "* **Formula:** $\\text{Gap}_1 = J_{\\text{train}} - \\text{Baseline Error}$\n",
    "* **Interpretation:** If $\\text{Gap}_1$ is **LARGE**, it means the algorithm is not even fitting the training data as well as reasonably expected. **$\\implies$ High Bias Problem.**\n",
    "* **Example 1 (Speech):** HLP is 10.6%, $J_{\\text{train}}$ is 10.8%. $\\text{Gap}_1 = 0.2\\%$. This is small, meaning the algorithm is fitting the training data well **relative to the baseline**.\n",
    "\n",
    "#### Gap 2: Training Error to Cross-Validation Error ($J_{\\text{train}} \\rightarrow J_{\\text{cv}}$)\n",
    "This gap indicates the severity of the **High Variance** (Overfitting) problem.\n",
    "\n",
    "* **Formula:** $\\text{Gap}_2 = J_{\\text{cv}} - J_{\\text{train}}$\n",
    "* **Interpretation:** If $\\text{Gap}_2$ is **LARGE** ($J_{\\text{cv}}$ is much larger than $J_{\\text{train}}$), it means the algorithm is generalizing poorly. **$\\implies$ High Variance Problem.**\n",
    "* **Example 1 (Speech):** $J_{\\text{train}}$ is 10.8%, $J_{\\text{cv}}$ is 14.8%. $\\text{Gap}_2 = 4.0\\%$. This is large, indicating a **High Variance Problem.**\n",
    "\n",
    "### 3. Concluding the Diagnosis\n",
    "\n",
    "| Example Case | HLP / Baseline | $J_{\\text{train}}$ | $J_{\\text{cv}}$ | Bias Gap ($\\text{HLP} \\rightarrow J_{\\text{train}}$) | Variance Gap ($J_{\\text{train}} \\rightarrow J_{\\text{cv}}$) | Conclusion |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| **Case 1** | 10.6% | 10.8% | 14.8% | Small (0.2%) | Large (4.0%) | **High Variance** |\n",
    "| **Case 2** | 10.6% | 15.0% | 15.4% | Large (4.4%) | Small (0.4%) | **High Bias** |\n",
    "| **Case 3** | 10.6% | 15.0% | 19.7% | Large (4.4%) | Large (4.7%) | **High Bias & High Variance** |\n",
    "\n",
    "* **Final Rule of Thumb:**\n",
    "    * **High Bias** is confirmed if $J_{\\text{train}}$ is large **relative to the baseline**.\n",
    "    * **High Variance** is confirmed if $J_{\\text{cv}}$ is large **relative to $J_{\\text{train}}$**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e9f322-ef72-45a4-9e6b-b84dc1b44dfa",
   "metadata": {},
   "source": [
    "## Learning curves\n",
    "\n",
    "Here we introduces **Learning Curves** as a powerful diagnostic tool to understand a model's performance by plotting the training and cross-validation errors as a function of the **training set size ($m_{\\text{train}}$)**.\n",
    "\n",
    "\n",
    "### 1. What a Learning Curve Shows\n",
    "\n",
    "* **General Trends (for a good model):**\n",
    "    * **$J_{\\text{cv}}$ (Cross-Validation Error):** Always **decreases** as $m_{\\text{train}}$ increases (more data means a better, more generalized model).\n",
    "    * **$J_{\\text{train}}$ (Training Error):** Always **increases** as $m_{\\text{train}}$ increases. It's easy to fit 1-3 examples perfectly (error $\\approx 0$), but harder to fit hundreds perfectly.\n",
    "    * **Relationship:** $J_{\\text{cv}}$ is always higher than $J_{\\text{train}}$ because the parameters are fitted to the training set.\n",
    "\n",
    "### 2. Learning Curve for High Bias (Underfitting)\n",
    "* **Diagnosis:** The flattened error is significantly **above the desired Baseline Level of Performance** (e.g., human-level performance). The model is too simple (e.g., a straight line) for the data.\n",
    "* **Implication for Improvement:** **Getting more training data WILL NOT help much.** Because the model is too simple, adding more data will not change the simple function being fitted, and the errors will remain high. To fix high bias, you must increase model complexity.\n",
    "\n",
    "---\n",
    "\n",
    "![High bias](images/learning_curve_bias.png)\n",
    "\n",
    "\n",
    "### 3. Learning Curve for High Variance (Overfitting)\n",
    "* **Diagnosis:** The model is too complex and is overfitting the noise in the training data, leading to poor generalization.\n",
    "* **Implication for Improvement:** **Getting more training data IS LIKELY to help.** By extending the curve to the right:\n",
    "    * $J_{\\text{train}}$ will continue to rise.\n",
    "    * $J_{\\text{cv}}$ will continue to fall and approach $J_{\\text{train}}$.\n",
    "    * The gap closes, and the cross-validation error moves closer to the desired baseline performance.\n",
    "\n",
    "---\n",
    "![High variance](images/learning_curve_variance.png)\n",
    "\n",
    "\n",
    "### 4. Practical Considerations\n",
    "* **Computational Cost:** Plotting learning curves by training many models on different subsets of the data can be **computationally expensive** and is not done often in practice.\n",
    "* **Mental Model:** However, having the **mental picture** of the learning curves is essential for understanding whether your primary problem is high bias or high variance, which then guides your strategy for improving the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b40712-a266-48cf-859c-58d196e222db",
   "metadata": {},
   "source": [
    "## What to do next??\n",
    "\n",
    "The diagnosis (checking $J_{\\text{train}}$ and $J_{\\text{cv}}$) guides the choice of solution. Different strategies address either a High Bias (Underfitting) or a High Variance (Overfitting) problem.\n",
    "\n",
    "### 1. Strategies for High Variance (Overfitting)\n",
    "\n",
    "A high variance model is too complex and fits the training data (and noise) too well, leading to a large gap between $J_{\\text{train}}$ and $J_{\\text{cv}}$. The goal is to **simplify the model** or **provide more data**.\n",
    "\n",
    "* **Get More Training Examples:** $\\uparrow$ Data is the most effective way to reduce variance, as it forces the model to generalize better across a wider range of examples.\n",
    "* **Try a Smaller Set of Features:** $\\downarrow$ Features reduces the model's flexibility to find complex, \"wiggly\" functions, thereby simplifying the model.\n",
    "* **Increase Regularization ($\\lambda$):** $\\uparrow \\lambda$ forces the parameters ($W$) to be smaller (closer to zero), resulting in a smoother, less complex function.\n",
    "\n",
    "### 2. Strategies for High Bias (Underfitting)\n",
    "\n",
    "A high bias model is too simple and fails to capture the complexity of the data, resulting in a high $J_{\\text{train}}$ (poor performance even on the training set). The goal is to **increase the model's complexity/power**.\n",
    "\n",
    "* **Get Additional Features:** Adding relevant, new features provides the model with more information to better predict the output.\n",
    "* **Add Polynomial Features:** Introducing terms like $x^2, x^3, x_1x_2$ allows a linear model to fit non-linear data (i.e., increases its complexity).\n",
    "* **Decrease Regularization ($\\lambda$):** $\\downarrow \\lambda$ reduces the penalty on the parameters, giving the model more freedom to fit the training data better.\n",
    "\n",
    "### 3. The Systematic Workflow\n",
    "\n",
    "1.  **Diagnose:** Look at the error metrics ($J_{\\text{train}}$ and $J_{\\text{cv}}$) and the **Baseline Performance** to determine the primary issue:\n",
    "    * **High Bias:** $J_{\\text{train}}$ is high (relative to baseline).\n",
    "    * **High Variance:** $J_{\\text{cv}}$ $\\gg J_{\\text{train}}$.\n",
    "2.  **Act:** Choose the corresponding set of solutions.\n",
    "    * *Self-Correction Note:* Do not try to fix high bias by reducing the training set size, as this drastically worsens the cross-validation error and overall performance.\n",
    "\n",
    "This systematic approach replaces trial-and-error, allowing developers to be far more effective in improving their machine learning systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c52bf1-8e67-4b69-a22c-e0a0d263e687",
   "metadata": {},
   "source": [
    "## Iterative loop of ML development\n",
    "\n",
    "### 1. The Iterative ML Development Loop\n",
    "Developing a successful machine learning system is rarely a one-shot process; it requires multiple iterations:\n",
    "1.  **Architecture & Data:** Decide on the model, data, and initial hyperparameters.\n",
    "2.  **Implement & Train:** Train the initial model (which seldom works perfectly right away).\n",
    "3.  **Diagnostics:** Analyze performance using tests like **Bias and Variance** and **Error Analysis** (to be covered next).\n",
    "4.  **Decide & Refine:** Based on diagnostic insights, make changes (e.g., change model size, adjust $\\lambda$, add/remove data or features).\n",
    "5.  **Iterate:** Repeat the loop until the desired performance is reached.\n",
    "\n",
    "### 2. Example: Email Spam Classification\n",
    "* **Goal:** Build a supervised learning algorithm where input $\\mathbf{X}$ (email features) predicts output $y$ (1 for spam, 0 for non-spam).\n",
    "* **Feature Construction (Bag of Words):** A common method is to select the top 10,000 words in a dictionary and create a feature vector where:\n",
    "    * $x_i = 1$ if the $i$-th word appears in the email.\n",
    "    * $x_i = 0$ if the $i$-th word does not appear.\n",
    "\n",
    "### 3. Multiple Improvement Ideas (and the Challenge of Prioritization)\n",
    "Once a baseline model is trained, several ideas for improvement emerge, and choosing the most fruitful path is crucial for efficiency:\n",
    "\n",
    "| Improvement Idea | Description |\n",
    "| :--- | :--- |\n",
    "| **Collect More Data** | Launching large projects (like \"honeypots\") to acquire more spam emails. |\n",
    "| **Sophisticated Routing Features** | Using information from the email header (the sequence of servers an email traveled through) to identify spam paths. |\n",
    "| **Sophisticated Body Features** | Normalizing text by treating variations (e.g., \"discounting\" and \"discount\") as the same word. |\n",
    "| **Misspelling Detection** | Creating algorithms to recognize deliberate misspellings (\"watches,\" \"m0rtgage\") used by spammers to defeat simple filters. |\n",
    "\n",
    "### 4. Diagnostics Guide Decisions\n",
    "* **Bias/Variance as a Filter:** Decisions must be guided by diagnostics. For example, if the model has **High Bias**, spending months on a honeypot project (collecting more data) will be a waste of time.\n",
    "* **Error Analysis:** The next key diagnostic, Error Analysis, will provide more detailed insights on *which types of errors* the model makes, further guiding feature engineering and architecture decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ebef5d-68f0-4b07-8707-ed91b1b3fe60",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "**Error Analysis** is the second most important diagnostic tool (after bias/variance) for guiding effective machine learning development. It involves manually inspecting misclassified examples to prioritize where to focus improvement efforts.\n",
    "\n",
    "### 1. What is Error Analysis?\n",
    "* **Definition:** Manually examining a set of examples that the learning algorithm **misclassified** (from the cross-validation set) to gain insight into the common types and sources of error.\n",
    "* **Procedure:**\n",
    "    1.  Get a set of misclassified examples (e.g., 100 out of 500 total CV errors). If the error set is too large, randomly **sample a subset** (e.g., 100-200 examples) for manual review.\n",
    "    2.  Manually inspect these examples and group them into **common, non-mutually exclusive categories** (e.g., pharmaceutical spam, phishing, deliberate misspellings, embedded image spam).\n",
    "    3.  **Count** the number of misclassified examples falling into each category.\n",
    "\n",
    "### 2. Prioritizing Work\n",
    "* The counts provide empirical data for prioritizing development work.\n",
    "* **High-Impact Categories:** Categories with a large number of misclassified examples (e.g., Pharmaceutical spam: 21/100, Phishing: 18/100) are the most promising areas to focus engineering effort.\n",
    "* **Low-Impact Categories:** Categories with few examples (e.g., Deliberate Misspellings: 3/100) are less critical. Even if a perfect solution is built for this category, it would only fix 3% of the current errors.\n",
    "* **Benefit:** Error analysis helps avoid spending significant time on problems that ultimately have little impact on overall performance (a mistake the speaker admits to making early in his career).\n",
    "\n",
    "### 3. Error Analysis Guides Solutions\n",
    "The insights gained from counting errors inspire specific, targeted improvements:\n",
    "\n",
    "* **Pharmaceutical Spam:** Might inspire collecting **more specific data** (only pharma spam emails) or creating **new features** related to specific drug names.\n",
    "* **Phishing/Password Theft:** Might inspire creating special code to extract features from **suspicious URLs** within the email body.\n",
    "\n",
    "### 4. Limitations\n",
    "* Error analysis is **easiest for tasks that humans are good at** (e.g., classifying emails, recognizing images) because a human can easily determine why the algorithm made a mistake.\n",
    "* It is **harder for tasks that humans struggle with** (e.g., predicting ad click-through rates), as a manual inspection is less insightful.\n",
    "\n",
    "### 5. Synergy with Bias/Variance\n",
    "Error analysis works in conjunction with bias/variance diagnostics:\n",
    "* **Bias/Variance** determines **if** a strategy (like collecting more data) will help.\n",
    "* **Error Analysis** determines **what kind of data** or **what kind of features** should be the focus of the engineering effort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ad09d5-67f6-4164-8007-236f54520680",
   "metadata": {},
   "source": [
    "## Adding data\n",
    "\n",
    "Here we provides several techniques for efficiently **adding, collecting, or creating data** to improve a machine learning model, particularly emphasizing a **data-centric approach** to development.\n",
    "\n",
    "### 1. Targeted Data Collection\n",
    "* **Efficiency:** Instead of indiscriminately collecting \"more data of everything\" (which is slow and expensive), use **Error Analysis** to identify and target specific error subsets.\n",
    "* **Example:** If errors are dominated by **pharmaceutical spam**, focus efforts on labeling or acquiring more examples *only* of pharma spam to give the algorithm a targeted performance boost.\n",
    "\n",
    "### 2. Data Augmentation\n",
    "* **Definition:** Taking existing training examples ($\\mathbf{X}, \\mathbf{y}$) and applying **distortions or transformations** to the input ($\\mathbf{X}$) to generate new, unique training examples that share the original label ($\\mathbf{y}$).\n",
    "* **Image Data (OCR):**\n",
    "    * Apply basic distortions: **Rotation, scaling (enlarging/shrinking), or adjusting contrast**.\n",
    "    * Apply advanced distortions: **Random grid warping** to create a richer variety of examples.\n",
    "* **Audio Data (Speech Recognition):**\n",
    "    * Add realistic **background noise** (e.g., crowd noise, car noise).\n",
    "    * Simulate **recording quality issues** (e.g., bad cell phone connection).\n",
    "* **Guiding Principle:** Distortions should be **representative of the types of noise or variations** expected in the test set. Adding purely random or unrealistic noise is generally not helpful.\n",
    "\n",
    "### 3. Data Synthesis\n",
    "* **Definition:** Creating entirely **new examples from scratch** rather than modifying existing ones.\n",
    "* **Example (Photo OCR):** Generating images of text by using a computer's text editor, varying **fonts, colors, contrasts, and backgrounds**.\n",
    "* **Benefit:** Can quickly generate a very large number of realistic-looking training examples, particularly effective for computer vision tasks.\n",
    "* **Cost:** Requires significant initial effort to write the code to generate high-quality, realistic synthetic data.\n",
    "\n",
    "### 4. Shifting to a Data-Centric Approach\n",
    "* **Model-Centric (Traditional):** Holding the dataset fixed and focusing effort on improving the model/code (e.g., trying different algorithms, optimizing code).\n",
    "* **Data-Centric (Modern Focus):** Holding the model/algorithm (e.g., neural network) fixed and focusing effort on **engineering the data** used by the algorithm.\n",
    "* **Rationale:** Modern algorithms (neural networks, logistic regression, etc.) are already highly effective. Therefore, focusing on getting **high-quality, targeted data** is often the more efficient path to improving performance.\n",
    "\n",
    "### 5. Preview: Transfer Learning\n",
    "* **Context:** A technique for scenarios where data is scarce and hard to collect.\n",
    "* **Concept:** Using data and knowledge learned from a **different, often unrelated task** to boost the performance of the current, data-poor application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a7d29a-44cd-43d7-9094-07c92ff5ac32",
   "metadata": {},
   "source": [
    "## Transfer learning: using data from a different task\n",
    "\n",
    "**Transfer Learning** is a powerful technique for improving model performance, especially when labeled data for a specific application is scarce. It involves leveraging knowledge gained from training a model on a large, related dataset.\n",
    "\n",
    "### 1. The Transfer Learning Process\n",
    "\n",
    "Transfer learning consists of two main steps:\n",
    "\n",
    "1.  **Supervised Pre-training (Knowledge Acquisition):**\n",
    "    * Train a large neural network on a **very large, readily available dataset** (e.g., 1 million images with 1,000 different classes like cats, dogs, cars).\n",
    "    * This process teaches the network to extract **generic, low-level features** (edges, corners, curves, basic shapes) from the input data.\n",
    "    * Researchers often **post these pre-trained models online** so others can skip this time-consuming step.\n",
    "2.  **Fine-Tuning (Application-Specific Adaptation):**\n",
    "    * Take the pre-trained network and **replace the output layer** with a new, smaller output layer corresponding to your specific task (e.g., 10 units for handwritten digits 0-9).\n",
    "    * **Initialize** the weights ($W, b$) of the lower layers using the pre-trained values.\n",
    "    * **Train the new network** on your smaller, specific dataset. Two options exist:\n",
    "        * **Option 1 (Very Small Dataset):** Only train the parameters of the new output layer ($W^5, b^5$). **Hold the weights of the lower layers fixed.**\n",
    "        * **Option 2 (Slightly Larger Dataset):** Use the pre-trained weights as **initialization** and **train all the parameters** in the network.\n",
    "\n",
    "![Transfer Learning](images/transfer_learning.png)\n",
    "\n",
    "### 2. Why Transfer Learning Works\n",
    "\n",
    "* The early layers of a deep neural network learn features that are often **generic and transferable** across similar input types (e.g., edges and corners are useful for recognizing both cats and handwritten digits).\n",
    "* By starting with these pre-trained feature detectors, the new network begins training from a **much better starting point**, requiring far fewer examples to achieve good results on the new task.\n",
    "\n",
    "### 3. Key Constraints and Benefits\n",
    "\n",
    "* **Input Type Must Match:** The input data type ($\\mathbf{X}$) for pre-training and fine-tuning **must be the same**.\n",
    "    * Pre-trained on **images** $\\implies$ fine-tune on **images**.\n",
    "    * Pre-trained on **audio** $\\implies$ fine-tune on **audio**.\n",
    "    * Pre-trained on **text** $\\implies$ fine-tune on **text**.\n",
    "* **Data Scarcity Solution:** Transfer learning helps dramatically when the target application has a **small dataset** (e.g., getting good results with only 50 or 1,000 labeled images when pre-training used a million).\n",
    "* **Community Contribution:** Transfer learning thrives on the generous sharing of pre-trained models (like those behind concepts such as ImageNet, BERT, and GPT-3), allowing the entire machine learning community to build better systems faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ba7528-934e-4a45-8c08-e09135d0be08",
   "metadata": {},
   "source": [
    "## Full cycle of machine learning project\n",
    "\n",
    "### 1. The Iterative Development Stages\n",
    "Developing a valuable ML system involves a recurring cycle of stages:\n",
    "1.  **Project Scoping:** Defining the project and its goals (e.g., building speech recognition for mobile voice search).\n",
    "2.  **Data Collection:** Gathering the necessary input data (X) and corresponding labels (Y) for the task.\n",
    "3.  **Model Training & Iteration:** Training the model, performing **Error Analysis** and **Bias-Variance Diagnostics**, and iteratively improving performance.\n",
    "    * This stage often loops back to the Data Collection step (e.g., using data augmentation to get more **car-noise speech data** after error analysis reveals poor performance in that domain).\n",
    "4.  **Deployment (Production):** Making the model available for users to utilize.\n",
    "5.  **Monitoring & Maintenance:** Continuously tracking the system's performance and updating the model as needed.\n",
    "\n",
    "---\n",
    "\n",
    "![Fulle Cyle of ML Project](images/full_cycle_ml_project.png)\n",
    "\n",
    "### 2. Deployment in Production\n",
    "Deployment typically involves setting up an **Inference Server** to host the trained model and serve predictions:\n",
    "* **API Calls:** A user-facing application (e.g., a mobile app) sends the input ($\\mathbf{X}$, like an audio clip) via an API call to the server.\n",
    "* **Prediction:** The server runs the machine learning model and returns the prediction ($\\hat{Y}$, like the text transcript).\n",
    "* **Software Engineering:** Deployment requires significant software engineering effort to ensure the system is:\n",
    "    * **Reliable and Efficient:** Making fast, accurate predictions.\n",
    "    * **Scalable:** Managing large numbers of users.\n",
    "\n",
    "---\n",
    "\n",
    "![ML Deployment](images/ml_deployment.png)\n",
    "\n",
    "### 3. Monitoring and Maintenance\n",
    "Maintaining a deployed ML system is critical because the real-world data distribution can change:\n",
    "* **Logging:** Logging input data ($\\mathbf{X}$) and predictions ($\\hat{Y}$) is essential (while respecting privacy and consent).\n",
    "* **System Monitoring:** Logs are used to monitor the system's performance. For example, monitoring can reveal that the system is performing poorly on new search terms (e.g., names of new celebrities or politicians) that weren't in the original training data.\n",
    "* **Model Updates:** If monitoring detects a performance drop (data shift), the model must be retrained and updated to replace the older version.\n",
    "\n",
    "### 4. MLOps (Machine Learning Operations)\n",
    "* **Definition:** MLOps is a growing field that focuses on the systematic practices for **building, deploying, and maintaining** machine learning systems in a production environment.\n",
    "* **Focus:** Ensuring reliability, scalability, cost-efficiency, logging, and monitoring for ML models serving a large user base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67333e07-60ce-4466-ba2e-4d49cb359609",
   "metadata": {},
   "source": [
    "## Fairness, bias, and ethics\n",
    "\n",
    "This section emphasizes the critical importance of **ethics, fairness, and bias** in machine learning development, providing practical steps and general guidance for mitigating potential negative impacts.\n",
    "\n",
    "### 1. The Problem of Bias in ML Systems\n",
    "The history of machine learning includes documented cases of unacceptable bias leading to harm:\n",
    "* **Discrimination:** Systems have been shown to **discriminate against subgroups** (e.g., a hiring tool biased against women, bank loan approval systems biased against certain groups).\n",
    "* **Inaccurate Recognition:** Face recognition systems have demonstrated **higher error rates for dark-skinned individuals**, leading to misidentification.\n",
    "* **Reinforcing Stereotypes:** Algorithms can unintentionally **reinforce negative societal stereotypes** (e.g., search results for certain professions failing to show diversity).\n",
    "\n",
    "### 2. Negative and Adverse Use Cases\n",
    "* **Deepfakes:** Technology can be used to generate **fake videos** or content without consent and proper disclosure.\n",
    "* **Harmful Content:** Social media algorithms, by optimizing solely for engagement, have contributed to the **spreading of toxic or incendiary speech**.\n",
    "* **Fraud and Misuse:** Machine learning is used by malicious actors to create **fake content**, commit fraud, or build harmful products. Developers are urged to **walk away** from projects deemed unethical.\n",
    "\n",
    "### 3. General Guidance for Ethical ML Development\n",
    "While a simple ethical checklist does not exist, developers should incorporate these practices:\n",
    "* **Assemble a Diverse Team:** Prior to deployment, assemble a team diverse in gender, ethnicity, culture, and other dimensions to **brainstorm possible harms**, particularly to vulnerable groups. Diverse teams are better at spotting potential problems.\n",
    "* **Literature Search:** Conduct research on **industry standards or emerging guidelines** relevant to the application (e.g., standards for fairness in financial loan approval systems).\n",
    "* **Audit Against Harm:** Before deployment, **audit the system** to measure performance across identified dimensions of possible bias (e.g., measure accuracy for different genders or ethnicities) to ensure problems are identified and fixed.\n",
    "* **Develop a Mitigation Plan:** Create a plan for rapid response if harm occurs after deployment (e.g., a plan to roll back to a known fair system).\n",
    "* **Monitor Post-Deployment:** **Continuously monitor** the system for unexpected harm or ethical failures in production to allow for quick execution of the mitigation plan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6110885-f82e-4b04-b012-6c3a70f6005e",
   "metadata": {},
   "source": [
    "## Error metrics for skewed datasets\n",
    "\n",
    "When dealing with **skewed datasets** (where one class is much rarer than the other), standard **accuracy** is an inadequate performance metric because a simple, non-learning model that always predicts the majority class can achieve artificially high accuracy.\n",
    "\n",
    "### 1. The Problem with Accuracy\n",
    "* **Example:** If a disease is present in only 0.5% of the population, a \"dummy\" algorithm that **always predicts \"no disease\"** ($y=0$) achieves 99.5% accuracy (0.5% error).\n",
    "* **Diagnosis Issue:** This makes it impossible to tell if a learning algorithm that achieves, say, 99% accuracy is any better than the useless dummy algorithm.\n",
    "\n",
    "### 2. The Solution: Precision and Recall\n",
    "To properly evaluate performance on skewed datasets, we use metrics derived from the **Confusion Matrix**, which categorize a classifier's predictions:\n",
    "\n",
    "| Actual Class | Predicted Class 1 (Positive) | Predicted Class 0 (Negative) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Actual Class 1** | **True Positive (TP)** | **False Negative (FN)** |\n",
    "| **Actual Class 0** | **False Positive (FP)** | **True Negative (TN)** |\n",
    "\n",
    "### 3. Defining Precision and Recall\n",
    "\n",
    "These two metrics, which must both be high for a useful model, are defined as follows:\n",
    "\n",
    "| Metric | Definition | Interpretation |\n",
    "| :--- | :--- | :--- |\n",
    "| **Precision** | $\\text{TP} / (\\text{TP} + \\text{FP})$ | **Of all the times the model predicted positive**, what fraction was correct? (Measures prediction accuracy.) |\n",
    "| **Recall** | $\\text{TP} / (\\text{TP} + \\text{FN})$ | **Of all the actual positive cases**, what fraction did the model correctly detect? (Measures completeness/sensitivity.) |\n",
    "\n",
    "![Confusion Matrix](images/confusion_matrix.png)\n",
    "\n",
    "### 4. Detecting Useless Algorithms\n",
    "* **Zero Prediction:** An algorithm that always predicts the negative class ($y=0$) will have **zero True Positives (TP)**.\n",
    "* **Recall Check:** Since $\\text{Recall} = 0 / (0 + \\text{FN})$, the recall will be **zero**, immediately signaling that the model is useless (i.e., it failed to detect any actual positive cases). This avoids the deception of high accuracy.\n",
    "\n",
    "### 5. Goal for Rare Classes\n",
    "When working with a rare class, the goal is to build a classifier that achieves **high values for both precision and recall**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd9b66c-bb75-4f98-9c84-b7ff8ad2b3f8",
   "metadata": {},
   "source": [
    "## Trading off precision and recall\n",
    "\n",
    "Here we discuss the necessity of **trade-off between Precision and Recall** when dealing with classification problems, particularly those with skewed datasets. It shows how the choice of an output threshold affects this balance and introduces the **F1 score** as a single metric to optimize the tradeoff.\n",
    "\n",
    "### 1. The Precision-Recall Trade-Off\n",
    "\n",
    "* **Ideal Goal:** High **Precision** (when the model predicts positive, it's usually correct) and high **Recall** (the model successfully finds most actual positive cases).\n",
    "* **The Dilemma:** In practice, improving one often comes at the expense of the other.\n",
    "\n",
    "##### Adjusting the Threshold (Logistic Regression)\n",
    "The standard prediction threshold for a logistic model is $f(\\mathbf{x}) \\ge 0.5$. Adjusting this threshold changes the trade-off:\n",
    "\n",
    "| Threshold Adjustment | Impact on Prediction | Effect on P & R | Scenario Example |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Raise Threshold** (e.g., to $\\ge 0.7$ or $0.9$) | Predicts $y=1$ only when **very confident**. | $\\uparrow$ **Precision** (fewer false positives, predictions are more accurate) / $\\downarrow$ **Recall** (misses more true positives) | When treatment is invasive/expensive, prioritizing diagnosis accuracy. |\n",
    "| **Lower Threshold** (e.g., to $\\ge 0.3$) | Predicts $y=1$ even when **less confident**. | $\\downarrow$ **Precision** (more false positives) / $\\uparrow$ **Recall** (finds more true positives) | When leaving the disease untreated has severe consequences, prioritizing detection completeness. |\n",
    "\n",
    "![Precision Recall Curve](images/precision_recall_curve.png)\n",
    "\n",
    "### 2. Choosing the Best Model\n",
    "\n",
    "When comparing multiple models, having two metrics (Precision and Recall) makes selection difficult.\n",
    "\n",
    "#### The F1 Score\n",
    "To select the best model, it's helpful to combine Precision ($P$) and Recall ($R$) into a single score. The simple average is inadequate because it can favor a model with very low P or very low R (e.g., a model that always predicts $y=1$).\n",
    "\n",
    "* **F1 Score Definition:** The **harmonic mean** of Precision and Recall. It gives greater emphasis to whichever of $P$ or $R$ is the lower value, thereby penalizing models with a severe imbalance.\n",
    "* **Formula:**\n",
    "    $$\\text{F1 Score} = \\frac{1}{1/2(\\frac{1}{P} + \\frac{1}{R})} = 2 \\cdot \\frac{P \\cdot R}{P + R}$$\n",
    "* **Usage:** By computing the F1 Score for different algorithms or for different threshold choices, the developer can pick the one with the **highest F1 Score**, which represents the best balanced trade-off between Precision and Recall."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
