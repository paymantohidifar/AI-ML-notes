{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0378adb2-bdb0-433b-baea-fc951ac6420a",
   "metadata": {},
   "source": [
    "# Week 1: Neural Netwroks\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "* Get familiar with the diagram and components of a neural network.\n",
    "* Understand the concept of a \"layer\" in a neural network.\n",
    "* Understand how neural networks learn new features.\n",
    "* Understand how activations are calculated at each layer.\n",
    "* Learn how a neural network can perform classification on an image.\n",
    "* Use a framework, TensorFlow, to build a neural network for classification of an image.\n",
    "* Learn how data goes into and out of a neural network layer in TensorFlow.\n",
    "* Build a neural network in regular Python code (from scratch) to make predictions.\n",
    "* (Optional): Learn how neural networks use parallel processing (vectorization) to make computations faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada2103d-e89c-4d22-833c-194fa32ad7eb",
   "metadata": {},
   "source": [
    "## Inference in Code (TensorFlow)\n",
    "\n",
    "### Key Concepts and Framework\n",
    "\n",
    "* **TensorFlow:** One of the leading frameworks for implementing deep learning algorithms, used primarily for its efficiency and extensive ecosystem.\n",
    "* **Inference:** The process of feeding an input feature vector (**$x$**) through a trained neural network to generate a prediction (**$\\hat{y}$**).\n",
    "* **Application Example (Coffee Roasting):** A simplified binary classification problem where the network predicts if a batch of coffee will taste \"good\" (positive cross, $y=1$) or \"bad\" (negative cross, $y=0$) based on two input features: **Temperature** and **Duration**.\n",
    "\n",
    "### Implementing Inference in TensorFlow\n",
    "\n",
    "The process involves defining the layers and sequentially passing the activations from one layer to the next.\n",
    "\n",
    "1.  **Input Feature Vector ($x$):** The input is a numpy array containing the feature values (e.g., $x = [200, 17]$ for temperature and duration).\n",
    "\n",
    "2.  **Defining Layer 1 (Hidden Layer):**\n",
    "    * **Syntax:** `Layer_1 = tf.keras.layers.Dense(units=3, activation='sigmoid')`\n",
    "    * **Interpretation:** Creates a **dense layer** (the standard layer type used so far) with 3 hidden units and the sigmoid activation function.\n",
    "\n",
    "3.  **Forward Propagation for Layer 1:**\n",
    "    * **Syntax:** `a1 = Layer_1(x)`\n",
    "    * **Result:** The activation vector **$a_1$** is computed and contains three numbers, one for each unit.\n",
    "\n",
    "4.  **Defining Layer 2 (Output Layer):**\n",
    "    * **Syntax:** `Layer_2 = tf.keras.layers.Dense(units=1, activation='sigmoid')`\n",
    "    * **Interpretation:** Creates the output layer with a single unit and the sigmoid activation function (suitable for binary classification).\n",
    "\n",
    "5.  **Forward Propagation for Layer 2:**\n",
    "    * **Syntax:** `a2 = Layer_2(a1)`\n",
    "    * **Result:** The final activation value **$a_2$** (a single number, e.g., 0.8), which represents the model's confidence in the positive class.\n",
    "\n",
    "6.  **Prediction ($\\hat{y}$):** The final activation $a_2$ is optionally **thresholded** (usually at 0.5) to produce the binary prediction $\\hat{y}$ (1 or 0).\n",
    "\n",
    "The same layer construction (`tf.keras.layers.Dense`) can be extended to deep networks, such as those used for handwritten digit classification, by simply chaining more layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d19d0b-4fbf-4cd5-93b9-d26c8d833958",
   "metadata": {},
   "source": [
    "## TensorFlow vs Keras\n",
    "\n",
    "Keras and TensorFlow are **not the same**, but they are intimately related. Keras is an **API** (Application Programming Interface) that serves as the high-level, user-friendly interface for building and training neural networks, while TensorFlow is the **complete framework** that provides the low-level, high-performance computational engine.\n",
    "\n",
    "Think of TensorFlow as the **engine** of a powerful race car, and Keras as the user-friendly **dashboard and steering wheel** that makes the car easy to drive.\n",
    "\n",
    "### Key Differences and Relationship\n",
    "\n",
    "| Feature | TensorFlow | Keras |\n",
    "| :--- | :--- | :--- |\n",
    "| **Role/Level** | **Framework** (Low-Level) | **API** (High-Level Abstraction) |\n",
    "| **Primary Goal** | Provides the numerical computation tools, scalability, and flexibility (the \"backend\"). | Simplifies the process of creating, configuring, and training deep learning models (the \"frontend\"). |\n",
    "| **Usability** | More complex, requires more code, offers **fine-grained control** for advanced users and research. | Simple, intuitive, requires minimal code, ideal for **beginners** and **rapid prototyping**. |\n",
    "| **Integration** | The entire system. | Originally a standalone library, it is now the **official high-level API** within TensorFlow (`tf.keras`). |\n",
    "\n",
    "### The Modern Relationship\n",
    "\n",
    "Since the release of TensorFlow 2.0, the relationship has been streamlined:\n",
    "\n",
    "* **Integration:** Keras is now fully integrated into TensorFlow as `tf.keras`. When you use Keras layers or models, you are indirectly using TensorFlow's computational engine to perform the underlying math operations.\n",
    "* **Default Use:** For most practitioners, TensorFlow recommends using the Keras APIs by default because they simplify the workflow for model building, training, and deployment.\n",
    "* **Customization:** You use Keras for simplicity and speed, but you can always drop down to the lower-level TensorFlow APIs if you need complete, fine-grained control over every tensor operation.\n",
    "\n",
    "In short, when you code a neural network today, you are typically using **TensorFlow** (the framework) through **Keras** (the easy-to-use API)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e630b2-9962-4db3-bc15-4a20e5e2bed5",
   "metadata": {},
   "source": [
    "## Building a Neural Network using TensorFlow\n",
    "\n",
    "### 1. Building a Neural Network with `Sequential`\n",
    "\n",
    "  * **Sequential Model:** TensorFlow's `Sequential` function provides a simpler, more compact way to define a neural network by chaining layers together in a linear stack. This is the **standard convention** for coding in TensorFlow.\n",
    "  * **Contrasts Explicit Forward Prop:** Instead of manually creating `Layer_1`, computing `a1 = Layer_1(X)`, creating `Layer_2`, and computing `a2 = Layer_2(a1)`, you define the entire structure upfront.\n",
    "  * **Compact Code Convention:** Layers are typically defined and passed directly into the `Sequential` function, avoiding explicit assignment to individual layer variables (`Layer_1`, `Layer_2`).\n",
    "    ```python\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units=3, activation='sigmoid'),  # Layer 1\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')   # Layer 2\n",
    "    ])\n",
    "    ```\n",
    "\n",
    "### 2. Training and Inference (The Three Key Functions)\n",
    "\n",
    "Once the model is defined, TensorFlow handles the heavy lifting of training and making predictions.\n",
    "\n",
    "| Function | Purpose | Details |\n",
    "| :--- | :--- | :--- |\n",
    "| **`model.compile(...)`** | **Configuration** | Specifies the optimizer, loss function, and metrics for training (details covered in the next week). |\n",
    "| **`model.fit(X, Y)`** | **Training** | Takes the input data (`X`) and target labels (`Y`) and trains the neural network. TensorFlow handles the backpropagation and gradient descent steps. |\n",
    "| **`model.predict(X_new)`** | **Inference/Forward Prop** | Performs **forward propagation** on new input data (`X_new`) and outputs the network's predictions (e.g., the value of $a_L$), replacing the need for manual layer computations. |\n",
    "\n",
    "### 3. Focus on Understanding\n",
    "\n",
    "While libraries like TensorFlow and PyTorch allow building complex, state-of-the-art networks with just a few lines of code (e.g., `model.compile`, `model.fit`, `model.predict`), it's crucial to understand what is happening **under the hood**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03117c58-d22c-4fbd-84e1-ec6fb6ce0c8b",
   "metadata": {},
   "source": [
    "## TensorFlow code snippet\n",
    "\n",
    "### Data normalization\n",
    "\n",
    "Normalizing training data will help gradient descent converge faster.\n",
    "\n",
    "```python\n",
    "# Import necessery classses.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tenserflow.keras.layers import Dense, Normalization\n",
    "\n",
    "# Normalized data.\n",
    "norm_l = Normalization(axis=-1)\n",
    "norm_l.adapt(X) # learns mean and variance\n",
    "X_n = norm_l(X)\n",
    "```\n",
    "\n",
    "### Building a model\n",
    "\n",
    "The goal is to build 2-layer NN model with 3 units in first layer and 1 unit in second layer:\n",
    "\n",
    "```python\n",
    "\n",
    "tf.random.seed(1234) # applied to archieve consistent results.\n",
    "\n",
    "# Define the model.\n",
    "model = Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(2,)) # specifies expected shape of the input (number of features)\n",
    "        Dense(3, activation='sigmoid', name='layer1'),\n",
    "        Dense(1, activation='sigmoid', name='layer2')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Output description of the model.\n",
    "model.summary()\n",
    "\n",
    "# Extract initialized weights\n",
    "W1, b1 = model.get_layer('layer1').get_weights()\n",
    "W2, b2 = model.get_layer('layer2').get_weights()\n",
    "\n",
    "# Define a loss function ans specifies a compile optimization\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(Xt, Yt, epochs=10)\n",
    "\n",
    "# Predict new examples.\n",
    "X_testn = norm_l(X_test) # Normalize test data\n",
    "model.predict(X_testn)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5297892-88ff-4c98-918f-5606afb313f3",
   "metadata": {},
   "source": [
    "## A brief note on Adam optimization algorithm\n",
    "\n",
    "The Adam optimizer (short for Adaptive Moment Estimation) is currently one of the most popular and effective algorithms for training deep neural networks. It is the go-to default optimizer for many TensorFlow (and PyTorch) projects because it typically converges faster and often performs better than traditional methods like Stochastic Gradient Descent (SGD).\n",
    "\n",
    "Adam combines the best features of two simpler optimizers (Momentum and RMSProp) by tracking two key moments:\n",
    "\n",
    "1.  **First Moment (Momentum):** Tracks the average of past gradients. This helps **accelerate** learning in the correct direction and **dampens oscillations** (wobbles) in the loss landscape.\n",
    "2.  **Second Moment (Adaptive Scaling):** Tracks the average of the squared past gradients. This is used to **scale the learning rate**:\n",
    "    * Parameters that have historically seen large updates get a **smaller, conservative learning rate**.\n",
    "    * Parameters that have historically seen small updates get a **larger, more aggressive learning rate**.\n",
    "\n",
    "### Benefits\n",
    "\n",
    "* **Fast Convergence:** Typically reaches the optimal solution quicker than traditional methods like Stochastic Gradient Descent (SGD).\n",
    "* **Ease of Use:** It is robust and generally requires minimal tuning of its default hyperparameters.\n",
    "* **TensorFlow Integration:** Easily implemented when compiling a Keras model: `optimizer=tf.keras.optimizers.Adam(...)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a0a673-3c2c-450f-b093-fa9620a5f757",
   "metadata": {},
   "source": [
    "## Concenpt of an epoch\n",
    "\n",
    "That's a fundamental concept in machine learning! An **epoch** is simply one complete pass through the entire training dataset during the training of a machine learning model, typically a neural network.\n",
    "\n",
    "Imagine you have a recipe book with 1,000 different recipes, and you need to learn to cook all of them perfectly.\n",
    "\n",
    "* **Training Dataset:** The entire recipe book (all 1,000 recipes).\n",
    "* **One Epoch:** You cook all 1,000 recipes exactly once.\n",
    "\n",
    "During one epoch, the following sequence of events occurs for every single data point in your training set:\n",
    "\n",
    "1.  **Forward Pass:** The data point is fed through the neural network to generate a prediction.\n",
    "2.  **Loss Calculation:** The error (or \"loss\") between the prediction and the true label is calculated.\n",
    "3.  **Backward Pass:** The error is backpropagated through the network.\n",
    "4.  **Parameter Update:** The model's internal weights and biases are adjusted to reduce that error (using an optimizer like Adam).\n",
    "\n",
    "Once all data points have been processed and the model's parameters have been updated based on the entire set, **one epoch is complete.**\n",
    "\n",
    "### Why We Use Multiple Epochs\n",
    "\n",
    "Training a model usually takes many epochs (often tens or hundreds) because a single pass isn't enough for the model to fully learn the complex patterns in the data.\n",
    "\n",
    "* **Initial Epochs:** The model's error is high, and the weights change dramatically with each epoch as the model begins to learn the main features.\n",
    "* **Later Epochs:** The model starts to fine-tune its parameters, and the change in error (loss) becomes smaller with each subsequent pass.\n",
    "\n",
    "### Epochs vs. Iterations\n",
    "\n",
    "It's important to distinguish epochs from **iterations** (or **steps**), especially when using Mini-Batch Gradient Descent:\n",
    "\n",
    "| Term | Definition |\n",
    "| :--- | :--- |\n",
    "| **Epoch** | One complete pass through the **entire dataset** ($m$ examples). |\n",
    "| **Iteration (or Step)** | One pass through a **single mini-batch** of data ($m_{batch}$ examples) resulting in one parameter update. |\n",
    "\n",
    "If your training set has $m=10,000$ examples and your mini-batch size is $m_{batch}=100$, then:\n",
    "\n",
    "$$\\text{Iterations per Epoch} = \\frac{\\text{Total Examples (m)}}{\\text{Mini-Batch Size (m}_{batch})} = \\frac{10,000}{100} = 100$$\n",
    "\n",
    "So, **1 epoch** requires **100 iterations** to complete.\n",
    "\n",
    "### Stopping Training\n",
    "\n",
    "You generally don't train for an arbitrary number of epochs. You monitor metrics to decide when to stop:\n",
    "\n",
    "* **Convergence:** When the loss on the training set stops decreasing significantly.\n",
    "* **Overfitting:** A critical point is reached when the model begins to perform worse on new, unseen data (the validation set). Training beyond this point means the model is memorizing the training data, and you should stop (a technique known as **Early Stopping**)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
