{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbde13f9-5b80-4462-8462-f1b71dfbe82a",
   "metadata": {},
   "source": [
    "# Week 4: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f69d2d-bcac-4f72-9975-a8a47b642884",
   "metadata": {},
   "source": [
    "## Decision tree model\n",
    "\n",
    "Here, we introduce **decision trees** as a powerful and widely-used machine learning algorithm, using a **cat classification** example to illustrate the model's structure and function.\n",
    "\n",
    "### 1. Decision Trees: A Powerful Tool\n",
    "* Decision trees and tree ensembles (collections of trees) are highly effective algorithms, often used to win machine learning competitions, despite receiving less attention in academia.\n",
    "\n",
    "### 2. Running Example: Cat Classification\n",
    "* The example uses a dataset of 10 animals (5 cats, 5 dogs) to classify if an animal is a cat ($Y=1$) or not ($Y=0$).\n",
    "* **Features ($\\mathbf{X}$):** Ear shape (pointy/floppy), face shape (round/not round), and whiskers (present/absent).\n",
    "* **Feature Type:** In this initial example, features are **categorical** (taking on a few discrete values).\n",
    "\n",
    "### 3. Structure of a Decision Tree Model\n",
    "A trained decision tree model is a structure that looks like an inverted, hanging plant (root at the top, leaves at the bottom):\n",
    "* **Root Node:** The topmost node where classification begins.\n",
    "* **Decision Nodes (Ovals):** Intermediate nodes that contain a feature (e.g., ear shape). They decide which branch to follow based on the feature's value.\n",
    "* **Leaf Nodes (Rectangles):** The nodes at the bottom that make the final **prediction** (the classification output, e.g., \"Cat\" or \"Not Cat\").\n",
    "\n",
    "![Decision Tree](images/decision_tree.png)\n",
    "\n",
    "### 4. Making a Prediction (Inference)\n",
    "The classification process involves traversing the tree from the top down:\n",
    "1.  Start at the **root node**.\n",
    "2.  Check the feature specified at the current **decision node**.\n",
    "3.  Follow the branch (left or right) corresponding to the feature's value for the input example.\n",
    "4.  Repeat this process until a **leaf node** is reached.\n",
    "5.  The prediction written in the leaf node is the final classification (Cat or Not Cat).\n",
    "\n",
    "### 5. The Learning Goal\n",
    "* Many different decision trees can be constructed for a given dataset.\n",
    "* The job of the decision tree learning algorithm is to **select the specific tree** that performs well on the training data and **generalizes effectively** to new, unseen data (cross-validation/test sets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a4493-d6e2-4c05-94e7-578ea9fff019",
   "metadata": {},
   "source": [
    "## Learning Process\n",
    "\n",
    "This section outlines the **recursive process** used to construct a decision tree from a training dataset. It details the steps of splitting data based on features and discusses the key decisions—what feature to split on and when to stop splitting—that define the tree-building algorithm.\n",
    "\n",
    "### 1. The Recursive Tree-Building Process\n",
    "The decision tree is built top-down, recursively splitting the data at each node:\n",
    "\n",
    "1.  **Start at the Root Node:** Decide the first feature to split the entire dataset on (e.g., ear shape).\n",
    "2.  **Split the Data:** Divide the training examples into subsets based on the chosen feature's value (e.g., pointy ears go left, floppy ears go right).\n",
    "3.  **Recurse on Subsets:** Focus on one subset (a branch) and repeat the process: decide the next feature to split on (e.g., face shape) and further divide the subset.\n",
    "4.  **Create Leaf Nodes:** When a subset is \"pure\" (all one class) or meets a stopping criterion, create a leaf node to make the final classification prediction.\n",
    "\n",
    "### 2. Key Decision 1: Choosing the Split Feature (Maximizing Purity)\n",
    "At every decision node, the algorithm must choose the feature that best divides the data into purer subsets.\n",
    "\n",
    "* **Goal:** Choose a feature that maximizes **purity** (or minimizes **impurity**), resulting in left and right branches where the examples are as close as possible to being all one class (e.g., all cats or all dogs).\n",
    "* **Ideal Split:** A feature like \"Cat DNA\" (if available) would be perfect, resulting in $100\\%$ pure subsets immediately.\n",
    "* **Practical Choice:** The algorithm evaluates available features (ear shape, face shape, whiskers) and selects the one that leads to the greatest increase in purity in the resulting branches.\n",
    "\n",
    "### 3. Key Decision 2: Deciding When to Stop Splitting\n",
    "Stopping criteria are necessary to prevent the tree from becoming too complex, which helps manage its size and reduces the risk of **overfitting** (learning noise in the training data).\n",
    "\n",
    "Common criteria for creating a leaf node and stopping further splits include:\n",
    "\n",
    "* **Purity Reached:** The node's subset is $100\\%$ pure (all examples belong to one class).\n",
    "* **Maximum Depth:** The tree reaches a predefined maximum depth, a hyperparameter set by the user (e.g., max depth of 2).\n",
    "* **Purity Improvement Threshold:** The gains in purity from splitting the node are below a certain minimum threshold.\n",
    "* **Minimum Examples:** The number of training examples at the node falls below a specific threshold (e.g., stop if there are fewer than 5 examples).\n",
    "\n",
    "### 4. Note on Decision Tree Complexity\n",
    "The decision tree algorithm often feels complex because it has evolved over time, with various researchers adding different refinements (like multiple stopping criteria) to improve its effectiveness. Despite its apparent complexity, these pieces combine to form a highly effective learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2f12dd-67e8-4acc-9b18-0b1a382fdc41",
   "metadata": {},
   "source": [
    "## Measuring Purity\n",
    "\n",
    "Here, we introduce **Entropy ($H$)** as the primary measure of **impurity** in a set of training examples, a critical concept used in decision trees to determine the best way to split data.\n",
    "\n",
    "### 1. Definition of Impurity\n",
    "* **Purity:** A set of examples is considered \"pure\" if all examples belong to a single class (e.g., all cats or all not-cats/dogs).\n",
    "* **Impurity:** The measure of how mixed the classes are within a set of data.\n",
    "\n",
    "### 2. The Entropy Function ($H$)\n",
    "Entropy is the function used to quantify impurity, conventionally denoted as $H(p_1)$, where $p_1$ is the fraction of positive examples (cats, or label $Y=1$) in the sample.\n",
    "\n",
    "* **Formula:**\n",
    "    $$H(p_1) = -p_1 \\log_2(p_1) - p_0 \\log_2(p_0)$$\n",
    "    Where $p_0$ is the fraction of negative examples ($1 - p_1$).\n",
    "\n",
    "* **Convention:** Logarithms are taken to base 2 ($\\log_2$) so that the maximum entropy value is exactly 1. By convention, $0 \\log(0)$ is treated as 0.\n",
    "\n",
    "### 3. Interpreting Entropy Values\n",
    "| Condition | $p_1$ (Fraction of Positive Examples) | Entropy ($H$) | Interpretation |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Maximum Impurity (Most Mixed)** | $0.5$ (e.g., 3 cats and 3 dogs) | $1$ | The set is a perfect $50-50$ mix. |\n",
    "| **Minimum Impurity (Pure)** | $1.0$ (e.g., all cats) | $0$ | The set is perfectly pure. |\n",
    "| **Minimum Impurity (Pure)** | $0.0$ (e.g., all dogs) | $0$ | The set is perfectly pure. |\n",
    "\n",
    "* **Relationship:** Entropy is highest when the classes are equally mixed ($p_1=0.5$) and decreases as the sample becomes purer (closer to $p_1=0$ or $p_1=1$). \n",
    "\n",
    "### 4. Alternative Measures\n",
    "* **Gini Criterion (Gini Impurity):** This is another function that looks similar to the entropy curve and is also widely used in decision tree packages to measure impurity.\n",
    "* **Focus:** For simplicity, the instruction focuses primarily on using the **entropy criteria** as it works well for most applications.\n",
    "\n",
    "### 5. Next Step\n",
    "The next section will demonstrate how to practically use this definition of **entropy** to make the key decision in the tree-building process: **which feature to split on at each node.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23232cda-9038-4426-92f3-6ef3c2248667",
   "metadata": {},
   "source": [
    "## Choosing a split: Information Gain\n",
    "\n",
    "When building a decision tree, the decision of which feature to split on at any node is made by calculating the **information gain** for each available feature. The split that yields the maximum information gain is chosen because it results in the greatest reduction in impurity (entropy).\n",
    "\n",
    "### 1. Information Gain Defined\n",
    "* **Information Gain** is the metric used to select the best feature for splitting a node in a decision tree.\n",
    "* It measures the **reduction in entropy (impurity)** achieved by performing a split using a specific feature.\n",
    "* **Goal:** The algorithm chooses the feature that results in the **highest Information Gain**.\n",
    "\n",
    "### 2. The Information Gain Formula\n",
    "Information gain is calculated as the entropy of the parent node minus the weighted average entropy of the resulting child nodes (the left and right branches):\n",
    "\n",
    "$$\\text{Information Gain} = H(p_{1}^{\\text{root}}) - \\left( w^{\\text{left}} \\cdot H(p_{1}^{\\text{left}}) + w^{\\text{right}} \\cdot H(p_{1}^{\\text{right}}) \\right)$$\n",
    "\n",
    "| Term | Definition | Example (Root Node) |\n",
    "| :--- | :--- | :--- |\n",
    "| **$H(p_{1}^{\\text{root}})$** | Entropy of the parent node (e.g., $H(0.5) = 1$ for a 50/50 mix). | $1$ |\n",
    "| **$w^{\\text{left}}$** | Fraction of examples that went to the left branch. | $5/10$ |\n",
    "| **$H(p_{1}^{\\text{left}})$** | Entropy of the left child node. | $H(0.8) \\approx 0.72$ (for 4 out of 5 cats) |\n",
    "| **$w^{\\text{right}}$** | Fraction of examples that went to the right branch. | $5/10$ |\n",
    "| **$H(p_{1}^{\\text{right}})$** | Entropy of the right child node. | $H(0.2) \\approx 0.72$ (for 1 out of 5 cats) |\n",
    "\n",
    "### 3. Practical Example: Selecting the Root Node Split\n",
    "When evaluating three possible features for the root node (initial entropy $H=1$):\n",
    "\n",
    "| Split Feature | Weighted Average Entropy (Child Nodes) | Information Gain | Decision |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Ear Shape** | $(5/10 \\cdot 0.72) + (5/10 \\cdot 0.72) = 0.72$ | $1 - 0.72 = \\mathbf{0.28}$ | **Chosen** (Highest Gain) |\n",
    "| **Face Shape** | $(7/10 \\cdot 0.99) + (3/10 \\cdot 0.92) \\approx 0.96$ | $1 - 0.96 = 0.03$ | Not chosen |\n",
    "| **Whiskers** | $(4/10 \\cdot 0.81) + (6/10 \\cdot 0.65) \\approx 0.71$ | $1 - 0.71 = 0.12$ | Not chosen |\n",
    "\n",
    "* The **Ear Shape** feature provides the largest $\\mathbf{0.28}$ reduction in entropy (information gain) and is therefore selected as the split for the root node.\n",
    "\n",
    "![Information Gain](images/information_gain.png)\n",
    "\n",
    "### 4. Role in Stopping Criteria\n",
    "* Information gain is not just used for feature selection; it is also critical for the stopping criteria.\n",
    "* A decision tree can be stopped from splitting further if the calculated **Information Gain is below a certain threshold**, preventing the tree from growing unnecessarily large and becoming prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c12cb-b1b7-4eb2-8bf6-d6d44d093d94",
   "metadata": {},
   "source": [
    "## Steps for Building a Decision Tree\n",
    "\n",
    "This section describes the complete, **recursive algorithm** for building a decision tree, using the previously defined **Information Gain** criterion and various **stopping criteria**.\n",
    "\n",
    "### 1. The Recursive Building Process\n",
    "The decision tree is constructed using a recursive, top-down approach:\n",
    "\n",
    "1.  **Start at the Root:** Begin with all training examples at the root node.\n",
    "2.  **Select Best Split:** Calculate the **Information Gain** for *all available features* and select the feature that yields the **highest gain**.\n",
    "3.  **Split the Data:** Divide the current set of examples into two subsets (left and right branches) based on the value of the selected feature.\n",
    "4.  **Recurse:** Apply the entire process (steps 2 and 3) independently and recursively on the left subset and the right subset.\n",
    "\n",
    "### 2. Stopping Criteria\n",
    "The recursive splitting process continues until one or more of the predefined stopping criteria are met at a node, at which point the node becomes a **leaf node** (making a final prediction):\n",
    "\n",
    "* **Purity:** The node's examples are $100\\%$ a single class (Entropy is zero).\n",
    "* **Maximum Depth:** Splitting the node would cause the tree to exceed a set maximum depth (a hyperparameter).\n",
    "* **Information Gain Threshold:** The information gain from any potential split is below a minimum threshold.\n",
    "* **Minimum Examples:** The number of training examples in the node is below a set threshold.\n",
    "\n",
    "### 3. Maximum Depth as a Hyperparameter\n",
    "* **Effect:** A larger maximum depth allows the tree to learn a more complex, high-degree model, but this **increases the risk of overfitting**.\n",
    "* **Tuning:** Max depth can theoretically be chosen using cross-validation, but open-source libraries often provide robust defaults or better methods for selection.\n",
    "\n",
    "### 4. Prediction (Inference)\n",
    "Once the tree is built, making a prediction for a new test example involves:\n",
    "1.  Starting at the root node.\n",
    "2.  Following the decision path determined by the example's feature values.\n",
    "3.  Stopping at the leaf node to retrieve the final class prediction.\n",
    "\n",
    "### 5. Next Focus\n",
    "The next steps in refining the algorithm involve learning how to handle features that are **categorical** but take on **more than two possible values** (e.g., more than just \"pointy\" or \"floppy\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eb6416-cb3e-4093-8315-10f4b872d65c",
   "metadata": {},
   "source": [
    "## Using one-hot encoding (OHE) of categorical features\n",
    "\n",
    "This section explains how to handle **categorical features** with more than two possible values in machine learning models, primarily by using **one-hot encoding**.\n",
    "\n",
    "### 1. The Challenge of Multi-Value Categorical Features\n",
    "* The basic decision tree algorithm relies on features that split data into two branches (binary features).\n",
    "* If a categorical feature (like ear shape) can take on **$k$ discrete values** (e.g., pointy, floppy, oval, so $k=3$), splitting on that feature naturally creates $k$ subsets and $k$ corresponding branches in the tree.\n",
    "\n",
    "### 2. Solution: One-Hot Encoding\n",
    "The recommended approach to address multi-value categorical features, especially for binary decision trees, is **one-hot encoding**.\n",
    "\n",
    "* **Process:** A single categorical feature that can take $k$ values is replaced by **$k$ new binary features** (dummy variables).\n",
    "* **Encoding:** For any given example, exactly **one** of these new features will have a value of **1** (\"hot\"), and the rest will be **0**.\n",
    "\n",
    "| Original Feature (Ear Shape) | Pointy Ear (New Feature 1) | Floppy Ear (New Feature 2) | Oval Ear (New Feature 3) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| Pointy | 1 | 0 | 0 |\n",
    "| Oval | 0 | 0 | 1 |\n",
    "| Floppy | 0 | 1 | 0 |\n",
    "\n",
    "* **Benefit for Decision Trees:** Once one-hot encoded, the decision tree algorithm can use the binary (0 or 1) features, applying the existing information gain criteria without modification.\n",
    "\n",
    "### 3. Broad Applicability\n",
    "* **Not just for Decision Trees:** One-hot encoding is a standard preprocessing technique that works for almost all machine learning models, including:\n",
    "    * **Neural Networks:** Converts categorical inputs into the numerical format (zeros and ones) that neural networks require.\n",
    "    * **Logistic/Linear Regression:** Allows these models to incorporate categorical information as numerical input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7cf4cf-a3b1-4535-a7e7-bd1906f5a198",
   "metadata": {},
   "source": [
    "## Continuous Valued Features\n",
    "\n",
    "This section explains how to adapt the decision tree algorithm to handle **continuous-valued features** (features that can take any numerical value, like weight) by identifying the optimal split threshold.\n",
    "\n",
    "### 1. Handling Continuous Features\n",
    "* The decision tree algorithm must consider continuous features (like **weight**) alongside categorical features (ear shape, whiskers) when determining the best split at any given node.\n",
    "* **Method:** To split on a continuous feature, the algorithm searches for an optimal **threshold value ($\\tau$)** to create a binary split:\n",
    "    * **Left Branch:** Examples where $\\text{Feature} \\leq \\tau$\n",
    "    * **Right Branch:** Examples where $\\text{Feature} > \\tau$\n",
    "\n",
    "### 2. Selecting the Optimal Threshold ($\\tau$)\n",
    "The goal is to choose the threshold $\\tau$ that maximizes **Information Gain**.\n",
    "\n",
    "* **Search Strategy:** Instead of trying every possible number, a standard convention is to:\n",
    "    1.  **Sort** all training examples based on the continuous feature's value (e.g., weight).\n",
    "    2.  Test the **midpoints** between consecutive, distinct values in the sorted list as candidate thresholds.\n",
    "    * *Example:* For 10 training examples, you would test 9 different possible threshold values.\n",
    "* **Information Gain Calculation:** For each candidate threshold $\\tau$, the algorithm calculates the Information Gain using the standard formula:\n",
    "    $$\\text{Information Gain}(\\tau) = H(p_1^{\\text{root}}) - \\left( w^{\\text{left}} \\cdot H(p_1^{\\text{left}}) + w^{\\text{right}} \\cdot H(p_1^{\\text{right}}) \\right)$$\n",
    "\n",
    "### 3. Split Decision\n",
    "* The algorithm compares the **maximum Information Gain** achieved by the best split on the continuous feature (e.g., $\\text{Weight} \\leq 9$ with $I.G.=0.61$) against the maximum Information Gain achieved by splitting on any of the categorical features.\n",
    "* **Decision:** If the continuous feature with its optimal threshold provides the highest Information Gain overall, that is the feature used for the split at the current node.\n",
    "\n",
    "### 4. Next Topic\n",
    "The core discussion concludes, noting that decision trees can also be generalized to solve **regression problems** (predicting a continuous number) rather than just classification (predicting a discrete category), a topic for the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9c7f3a-02c5-4128-a997-b78814f8ced9",
   "metadata": {},
   "source": [
    "## Regression Trees\n",
    "\n",
    "This section generalizes the decision tree algorithm from a **classification** technique (predicting a category like \"cat\" or \"not-cat\") to a **regression** technique (predicting a continuous number like \"weight\").\n",
    "\n",
    "### 1. The Regression Tree Model\n",
    "* **Target Output ($Y$):** The output is a **continuous number** (e.g., animal weight in pounds), transforming the problem into regression.\n",
    "* **Prediction at Leaf Nodes:** When a test example reaches a leaf node, the tree predicts a value calculated as the **average** of the target values ($Y$) of all training examples that landed in that same leaf node.\n",
    "\n",
    "### 2. Decision Criteria: Reducing Variance\n",
    "For regression problems, the tree-building algorithm replaces **entropy** with **variance** to measure impurity.\n",
    "\n",
    "* **Measure of Impurity:** The **variance** of the $Y$ values in a subset of data is used to quantify how widely the numbers vary. A pure subset for regression has low variance.\n",
    "* **Goal:** The algorithm seeks to maximize the **Reduction in Variance** (analogous to Information Gain).\n",
    "\n",
    "### 3. Reduction in Variance Formula\n",
    "The formula measures the decrease in variance from the parent node to the weighted average of the child nodes:\n",
    "\n",
    "$$\\text{Reduction in Variance} = \\text{Variance}_{\\text{root}} - \\left( w^{\\text{left}} \\cdot \\text{Variance}_{\\text{left}} + w^{\\text{right}} \\cdot \\text{Variance}_{\\text{right}} \\right)$$\n",
    "\n",
    "* **$\\text{Variance}_{\\text{root}}$:** The variance of the target values ($Y$) in the parent node (the entire dataset at the root).\n",
    "* **$w^{\\text{left/right}}$:** The fraction of examples sent to the left/right branch.\n",
    "* **$\\text{Variance}_{\\text{left/right}}$:** The variance of the target values ($Y$) in the resulting child nodes.\n",
    "\n",
    "### 4. Splitting Decision\n",
    "* At any node, the algorithm evaluates all possible feature splits (including the optimal threshold for continuous features).\n",
    "* **Selection Rule:** The feature that yields the **largest Reduction in Variance** is chosen for the split.\n",
    "\n",
    "### 5. Next Step\n",
    "While a single decision tree is functional, combining multiple decision trees into an **ensemble** (e.g., Random Forests or Boosting) can significantly improve prediction accuracy, which will be discussed next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0ce9c3-7e55-4c55-a0fa-7535e60c3e23",
   "metadata": {},
   "source": [
    "## Usign Multiple Decision Trees\n",
    "\n",
    "A significant weakness of using a single decision tree is its **high sensitivity** to small changes in the training data, leading to a phenomenon known as **instability**. The solution is to use a **tree ensemble**, which is a collection of multiple decision trees, to make the overall algorithm more robust and accurate.\n",
    "\n",
    "### 1. Weakness of a Single Decision Tree\n",
    "* **Instability:** A minor change, such as altering the features of a **single training example**, can completely change the optimal feature selected for the root split, resulting in a drastically different, and often suboptimal, final tree structure.\n",
    "* **Sensitivity:** This high sensitivity means the algorithm is **not robust** to noise or slight variations in the data.\n",
    "\n",
    "### 2. The Solution: Tree Ensemble\n",
    "* A **tree ensemble** (or collection of trees) is used to counter the instability of a single tree, yielding more accurate and stable predictions.\n",
    "\n",
    "### 3. Making Predictions via Voting\n",
    "* **Inference:** When a new test example is presented, it is passed through *every single tree* in the ensemble.\n",
    "* **Voting:** Each tree generates its own prediction (its \"vote\").\n",
    "* **Final Prediction:** The ensemble's final classification is determined by the **majority vote** of all the individual trees.\n",
    "\n",
    "### 4. Benefit of Ensembles\n",
    "* **Robustness:** Using many trees and having them vote makes the overall algorithm less sensitive to the specific behavior or potential error of any single tree.\n",
    "* **Accuracy:** This process smooths out errors and improves the generalization ability of the model.\n",
    "\n",
    "### 5. Next Step: Creating the Ensemble\n",
    "* The core challenge is generating multiple, **plausible but distinct** decision trees.\n",
    "* The next section will introduce **sampling with replacement** (a statistical technique) as the key method used to create the diverse datasets required to train the ensemble of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b54cc78-2c49-48e4-b468-e1ae0c9f43e8",
   "metadata": {},
   "source": [
    "## Sampling with Replacement\n",
    "\n",
    "To build a robust **tree ensemble** (a collection of multiple decision trees), a diverse set of training data must be generated for each tree. The technique used to create these varied datasets is **sampling with replacement**, also known as **bootstrapping**.\n",
    "\n",
    "### 1. Definition and Process\n",
    "* **Sampling with Replacement:** A statistical technique where an item is randomly selected from a set, recorded, and then **put back** into the set before the next selection is made.\n",
    "* **Result:** This process ensures that the same item can be selected multiple times, and conversely, some items may not be selected at all.\n",
    "\n",
    "### 2. Application to Tree Ensembles\n",
    "The technique is used to create multiple new, slightly different training sets from the original data, all while keeping the same total number of examples.\n",
    "\n",
    "* **Original Data:** Start with $m$ training examples (e.g., 10 examples).\n",
    "* **New Training Set:** A new training set is constructed by randomly selecting $m$ examples **with replacement** from the original set.\n",
    "* **Characteristics of the New Set:**\n",
    "    * **Same Size:** The new set has the same number of examples (e.g., 10) as the original set.\n",
    "    * **Duplicates:** The new set will likely contain **duplicate** examples from the original data (because selected examples are replaced).\n",
    "    * **Omissions:** The new set will likely **omit** some examples from the original data.\n",
    "\n",
    "### 3. Purpose\n",
    "* **Key Building Block:** Sampling with replacement is the fundamental technique used to generate the multiple, distinct training sets needed to train an ensemble of trees.\n",
    "* **Diversity:** By creating unique training sets (some with duplicates, some with omissions), this process introduces the necessary **variability** between the individual decision trees, which makes the final ensemble more robust and accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea383693-a240-4fd8-afa7-6ebbcddc265c",
   "metadata": {},
   "source": [
    "## Random Forest Algorithm: A Robust Tree Ensemble\n",
    "\n",
    "The **Random Forest** is a powerful tree ensemble algorithm that overcomes the high sensitivity and instability of a single decision tree by combining two core randomization techniques: **bootstrapping** (data randomness) and **feature randomness**.\n",
    "\n",
    "### 1. Ensemble Construction (Bootstrapping)\n",
    "The process starts by generating multiple, varied training sets to ensure each tree in the ensemble is trained slightly differently.\n",
    "\n",
    "* **Process:** For a desired number of trees ($B$, typically around 100), the algorithm performs the following $B$ times:\n",
    "    1.  **Bootstrapping:** Create a new training dataset of the same size ($M$) as the original, using **sampling with replacement** from the original data. This ensures the new set has duplicates and about one-third of the original data will typically be left out of any given bootstrap sample. The omitted data is known as the **Out-of-Bag (OOB)** sample and is often used for internal validation.\n",
    "    2.  **Train Tree:** Train a full decision tree on this new, unique dataset.\n",
    "* **Result (Bagged Decision Trees):** The initial ensemble of trees, sometimes called **Bagged Decision Trees** (with 'bag' referring to the virtual bag used for sampling).\n",
    "* **Tree Count ($B$):** While a larger $B$ never hurts performance, values around **64 to 128** are common, as increasing $B$ beyond this point offers diminishing returns and primarily increases computational time.\n",
    "\n",
    "### 2. Enhancing Diversity (Feature Randomness)\n",
    "To prevent all trees from using the same strongest features (which would make them highly correlated), the algorithm introduces randomness in the feature selection process.\n",
    "\n",
    "* **Mechanism:** At **every split** in every tree:\n",
    "    1.  **Random Subset:** Only a random subset of $K$ features is selected from the total available $N$ features.\n",
    "    2.  **Constrained Split:** The split is chosen by finding the highest Information Gain (or Reduction in Variance for regression) *only* among those $K$ features.\n",
    "* **Typical $K$ Value:** When the number of features ($N$) is large, a common heuristic is to set $K \\approx \\sqrt{N}$.\n",
    "* **Result (Random Forest):** This additional feature randomization step ensures that the final set of trees are **decorrelated** (different from one another), making the overall Random Forest much more robust and accurate than a bagged tree ensemble without this step.\n",
    "\n",
    "### 3. Making a Final Prediction\n",
    "* **Inference:** A new test example is run through all $B$ trees in the ensemble.\n",
    "* **Final Output:** The final prediction is determined by the **majority vote** of all the individual trees (for classification) or the average of their predictions (for regression).\n",
    "\n",
    "### 4. Overall Robustness\n",
    "The combination of data randomization (bootstrapping) and feature randomization makes the Random Forest highly robust, as the final prediction averages out the small variations and potential errors of the individual trees, making it less sensitive to noise or small changes in the training data.\n",
    "\n",
    "The next powerful ensemble technique to explore is **Boosted Decision Trees**, such as **XGBoost**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1613d2dd-c996-403d-83e7-1200c4fc1939",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "The section introduces **XGBoost (Extreme Gradient Boosting)** as the current state-of-the-art and most widely used implementation of tree ensembles, emphasizing its efficiency and superior performance over single decision trees and standard bagging. XGBoost achieves its high performance through a concept called **boosting**.\n",
    "\n",
    "### 1. The Core Idea: Boosting (Deliberate Practice)\n",
    "XGBoost is based on the concept of **boosting**, which is an iterative technique where new trees focus on the mistakes made by the previous ensemble of trees.\n",
    "\n",
    "* **Standard Bagging (Random Forest):** Treats all training examples equally when creating new datasets.\n",
    "* **Boosting (XGBoost Intuition):** Analogous to \"deliberate practice\" (like focusing on hard piano passages), new trees are built to give **higher attention (or weight)** to the training examples that the previously built trees **misclassified or handled poorly**.\n",
    "* **Goal:** Each subsequent tree attempts to correct the errors of the preceding trees, improving performance iteratively and quickly.\n",
    "\n",
    "### 2. Implementation Differences from Bagging\n",
    "* **Focus on Errors:** Instead of simply using sampling with replacement (bootstrapping), XGBoost internally **assigns different weights** to the training examples. Misclassified examples are given higher weights, forcing the next tree to prioritize learning those difficult cases.\n",
    "* **Efficiency:** Because XGBoost uses weights instead of generating entirely new random training sets via sampling, it can be more **efficient** than traditional bootstrapping methods.\n",
    "\n",
    "### 3. Key XGBoost Features (Transcript and Web)\n",
    "| Feature | Description |\n",
    "| :--- | :--- |\n",
    "| **Speed and Efficiency** | The open-source implementation is well-optimized to run very quickly, making it suitable for large-scale commercial use. |\n",
    "| **Superior Performance** | XGBoost is highly competitive and often used to **win machine learning competitions** (e.g., on Kaggle) alongside deep learning. |\n",
    "| **Gradient Boosting** | The \"GB\" in XGBoost stands for Gradient Boosting. It uses **gradient descent** to iteratively minimize a loss function, similar to how neural networks train, which is how it focuses on and corrects errors. |\n",
    "| **Regularization** | XGBoost includes built-in regularization terms ($\\mathbf{L1}$ and $\\mathbf{L2}$) in its objective function to **prevent overfitting**. This is a major innovation that distinguishes it from older boosting algorithms. |\n",
    "| **Flexibility** | It can be used for both **classification** (`XGBClassifier`) and **regression** (`XGBRegressor`) problems by simply changing the model initialization. |\n",
    "| **Handling Missing Data** | XGBoost has a built-in capability to handle missing values (NaNs) by automatically learning the best direction to send examples with missing data. |\n",
    "\n",
    "### 4. Implementation Note\n",
    "The mathematical details of exactly how the probabilities or weights are adjusted are complex, but practitioners can easily use the algorithm by importing the XGBoost library and initializing an `XGBClassifier` or `XGBRegressor`.\n",
    "\n",
    "```python\n",
    "# For classification problem\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# For regression problem\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "The boosting methods train several trees, but instead of them being uncorrelated to each other, now the trees are fit one after the other in order to minimize the error. \n",
    "\n",
    "The model has the same parameters as a decision tree, plus the learning rate.\n",
    "* The learning rate is the size of the step on the Gradient Descent method that the XGBoost uses internally to minimize the error on each train step.\n",
    "\n",
    "One interesting thing about the XGBoost is that during fitting, it can take in an evaluation dataset of the form `(X_val,y_val)`.\n",
    "* On each iteration, it measures the cost (or evaluation metric) on the evaluation datasets.\n",
    "* Once the cost (or metric) stops decreasing for a number of rounds (called early_stopping_rounds), the training will stop.\n",
    "* More iterations lead to more estimators, and more estimators can result in overfitting.\n",
    "* By stopping once the validation metric no longer improves, we can limit the number of estimators created, and reduce overfitting.\n",
    "\n",
    "We can then set a large number of estimators, because we can stop if the cost function stops decreasing.\n",
    "\n",
    "Note some of the `.fit()` parameters:\n",
    "* `eval_set = [(X_train_eval,y_train_eval)]`:Here we must pass a list to the eval_set, because you can have several different tuples ov eval sets. \n",
    "* `early_stopping_rounds`: This parameter helps to stop the model training if its evaluation metric is no longer improving on the validation set. It's set to 10.\n",
    "  * The model keeps track of the round with the best performance (lowest evaluation metric).  For example, let's say round 16 has the lowest evaluation metric so far.\n",
    "  * Each successive round's evaluation metric is compared to the best metric.  If the model goes 10 rounds where none have a better metric than the best one, then the model stops training.\n",
    "  * The model is returned at its last state when training terminated, not its state during the best round.  For example, if the model stops at round 26, but the best round was 16, the model's training state at round 26 is returned, not round 16.\n",
    "  * Note that this is different from returning the model's \"best\" state (from when the evaluation metric was the lowest)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c04df6-474a-4876-9488-7335d9acfde3",
   "metadata": {},
   "source": [
    "## When to Use Decision Trees\n",
    "\n",
    "In this section, we compare and contrast the strengths and weaknesses of **Decision Trees/Tree Ensembles** and **Neural Networks** to guide the choice between them for different types of machine learning problems.\n",
    "\n",
    "#### Decision Trees and Tree Ensembles (e.g., XGBoost, Random Forest)\n",
    "\n",
    "| Pros | Cons / Best For |\n",
    "| :--- | :--- |\n",
    "| **Tabular/Structured Data:** Highly effective and often competitive with neural networks for data stored in spreadsheets (e.g., house prices, customer records) with numerical or categorical features. | **Poor on Unstructured Data:** Not the preferred choice for complex data like **images, video, audio, or raw text**. |\n",
    "| **Fast Training:** Typically very quick to train, allowing for faster iteration through the machine learning development loop. | **Tree Ensemble Cost:** An ensemble (like XGBoost) is computationally more expensive than a single decision tree, though still faster than many neural networks. |\n",
    "| **Interpretability (Small Trees):** A small, single decision tree can be visually examined and understood by a human. | **Ensemble Interpretability:** The interpretability advantage is often **overstated**; an ensemble of 100+ trees is difficult to interpret. |\n",
    "| **Recommended Implementation:** For most applications, the preferred choice is a tree ensemble, specifically **XGBoost**, due to its speed, efficiency, and strong performance. | |\n",
    "\n",
    "### Neural Networks\n",
    "\n",
    "| Pros | Cons / Best For |\n",
    "| :--- | :--- |\n",
    "| **Unstructured Data (Preferred):** The **dominant and preferred algorithm** for complex data like **images, video, audio, and raw text**. | **Slower Training:** Large neural networks can be significantly slower to train than decision trees, hindering rapid iteration. |\n",
    "| **Universal Applicability:** Works well on all data types, including **structured, unstructured, and mixed data**. | **Complexity:** While not explicitly mentioned as a downside, they are typically more complex to design and tune than tree ensembles. |\n",
    "| **Transfer Learning:** Easily supports transfer learning (pre-training on a large dataset and fine-tuning on a small one), which is critical for applications with limited labeled data. | |\n",
    "| **System Integration:** Easier to integrate and train a system of **multiple interconnected machine learning models** using techniques like end-to-end gradient descent. | |\n",
    "\n",
    "### Conclusion on Choice\n",
    "\n",
    "* **For Tabular/Structured Data:** Both neural networks and tree ensembles (especially **XGBoost**) are highly competitive.\n",
    "* **For Unstructured Data:** **Neural networks** are the clear and necessary choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf8f7e6-55fb-4ae8-9bbf-1c286c814a8d",
   "metadata": {},
   "source": [
    "## Bonus: Feature Selection for Complex ML/DL problems\n",
    "\n",
    "## Feature Selection Methods\n",
    "\n",
    "Choosing the right features is a critical step in machine learning. While relying solely on **Pearson correlation** is a simple and fast starting point, it has significant limitations: it only measures the **linear relationship**, it **ignores non-linearity** (missing strong but non-linear relationships, e.g., $Y$ and $X^2$), it **ignores feature interactions** (missing predictive power from combinations of individually weak features), and it **doesn't account for redundancy** (multicollinearity).\n",
    "\n",
    "The standard, more robust approach to feature selection is to use methods categorized into three main groups: **Filter, Wrapper, and Embedded**.\n",
    "\n",
    "### Standard Feature Selection Techniques\n",
    "\n",
    "| Method Type | Mechanism | Examples | When to Use |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **1. Filter Methods** | Evaluate features based on their individual relationship with the label, independent of any ML model. | **Pearson's $r$** (linear correlation), **Chi-Squared Test** (for categorical features), and **Information Gain/Mutual Information** (for non-linear relevance). | As a quick, computationally cheap **initial screening** step to eliminate clearly irrelevant features. |\n",
    "| **2. Wrapper Methods** | Use a specific machine learning model (e.g., **SVM**) to score and evaluate subsets of features, treating selection as a search problem. | **Forward Selection**, **Backward Elimination**, and **Recursive Feature Elimination (RFE)**. | Provides superior, model-specific performance but is **computationally expensive**. **SVMs** are used here as the core estimator in RFE, where the magnitude of the model's weights ranks feature importance. |\n",
    "| **3. Embedded Methods** | Perform feature selection automatically *during* model training. | **Lasso ($\\mathbf{L1}$) Regularization** (forces coefficients toward zero in linear models) and **Tree Feature Importance** (e.g., in XGBoost or Random Forest, calculated via metrics like gain or weight). | Highly practical and efficient, balancing accuracy and speed as selection is integrated into training. |\n",
    "\n",
    "### Deep Learning (DL) for Feature Selection\n",
    "\n",
    "Deep learning can be used to either extract better, more compact features (**Representation Learning**) or perform selection inherently.\n",
    "\n",
    "| DL Approach | Mechanism | Techniques |\n",
    "| :--- | :--- | :--- |\n",
    "| **1. Feature Extractor** | Creates new, compressed, and more informative feature vectors from raw input data. | **Autoencoders** (unsupervised compression for tabular data via a bottleneck layer), **CNNs/Transformers** (automatically learn hierarchical features for unstructured data). |\n",
    "| **2. Embedded Selector** | Modifies the training process to score or penalize input features. | **L1 Regularization** (applied to weights connecting the input layer), **Dropout** (forces reliance on multiple inputs), and **Attention Mechanisms** (assign weights to inputs based on importance). |\n",
    "\n",
    "**Recommendation:** For **simple tabular data**, stick to XGBoost/Scikit-learn. For **high-dimensional tabular data**, use **Autoencoders**. For **unstructured data** (images, text), use **CNNs or Transformers**.\n",
    "\n",
    "### Best Practice Workflow\n",
    "\n",
    "A strong, standard workflow combines these methods:\n",
    "1.  **Filter:** Use a simple filter (like correlation or Chi-squared) to quickly discard clearly useless features.\n",
    "2.  **Embedded:** Use a powerful embedded method like **XGBoost's feature importance** to get a reliable ranking of the remaining features.\n",
    "3.  **Wrapper/Validation:** Test the top-ranked feature subsets using cross-validation on your final chosen model to ensure the best generalization performance.\n",
    "\n",
    "***\n",
    "\n",
    "### Support Vector Machines (SVMs)\n",
    "\n",
    "A **Support Vector Machine (SVM)** is a powerful and versatile **supervised machine learning algorithm** used primarily for **classification** (and regression via SVR).\n",
    "\n",
    "#### Core Concept: The Maximum Margin Hyperplane\n",
    "\n",
    "The goal of an SVM is to find the **hyperplane** (the decision boundary) that maximizes the **margin**—the distance between the boundary and the closest data points from either class.\n",
    "\n",
    "* **Support Vectors:** The few critical data points that lie closest to the hyperplane and define the margin. The SVM model is entirely defined by these points, which contributes to its **memory efficiency**.\n",
    "\n",
    "#### Handling Non-Linearity: The Kernel Trick\n",
    "\n",
    "The SVM's ability to handle **non-linearly separable** data comes from the **kernel trick**.\n",
    "\n",
    "* **Mechanism:** The kernel trick allows the SVM to implicitly map the input data into a much **higher-dimensional feature space** where the data often becomes linearly separable.\n",
    "* **Result:** A linear boundary in the higher space corresponds to a complex, non-linear boundary in the original space. **Common Kernels** include the Radial Basis Function (RBF) and polynomial kernels.\n",
    "\n",
    "#### Key SVM Advantages\n",
    "\n",
    "* Effective in **high-dimensional** spaces.\n",
    "* **Memory efficient** due to its reliance only on the **Support Vectors**.\n",
    "* **Versatile** for classification, Support Vector Regression (SVR), and outlier detection.\n",
    "\n",
    "***\n",
    "\n",
    "### Suggested Practical Learning Resources\n",
    "\n",
    "To effectively learn the standard feature selection methods using Python implementation:\n",
    "\n",
    "#### 1. Structured Course: DataCamp or Coursera\n",
    "\n",
    "* **DataCamp: Feature Engineering for Machine Learning in Python:** Excellent for its interactive approach, covering **feature creation** and applying filter/wrapper/embedded methods.\n",
    "\n",
    "#### 2. Practical Handbook: Scikit-learn Documentation\n",
    "\n",
    "* **Scikit-learn User Guide on Feature Selection:** The essential resource for learning the standard, clean Python implementation of methods like **Variance Thresholding**, **SelectKBest**, and **Recursive Feature Elimination (RFE)** (often with an SVM estimator).\n",
    "\n",
    "#### 3. Advanced/Embedded Resource: XGBoost Documentation\n",
    "\n",
    "* **XGBoost Feature Importance Tutorials:** Focuses on mastering the use of XGBoost's built-in **feature importance** metrics (`gain`, `weight`, `cover`), which represent a highly effective embedded feature selection method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08765be0-19c3-413c-a6e3-29a7ad03571e",
   "metadata": {},
   "source": [
    "## Bonus: Techniques for Measuring Feature Impact\n",
    "\n",
    "Determining which features have the most impact on a model's prediction (the label) is a critical area of machine learning known as **Model Interpretability** or **Explainable AI (XAI)**.\n",
    "\n",
    "Once a model is trained, several standard techniques can be used to quantify each feature's influence.\n",
    "\n",
    "***\n",
    "\n",
    "The methods used to determine feature impact generally fall into two categories: **Model-Specific** (techniques tied to a particular type of algorithm) and **Model-Agnostic** (techniques that can be applied to any trained model, regardless of its type).\n",
    "\n",
    "### 1. Model-Specific Techniques\n",
    "\n",
    "These methods are often the fastest and most direct way to get feature importance, provided you're using the right algorithm.\n",
    "\n",
    "* **Tree-Based Models (Random Forest, XGBoost):** These algorithms inherently track how often a feature is used to split a node and how much that split reduces impurity (Information Gain or Variance Reduction).\n",
    "    * **Metric:** **Feature Importance** (usually based on \"gain\" or \"Gini importance\"). Features with the highest gain are considered the most impactful.\n",
    "* **Linear Models (Linear/Logistic Regression):** The magnitude of the learned coefficients directly reflects the impact of the feature.\n",
    "    * **Metric:** **Coefficient Magnitude**. A large absolute coefficient value means a small change in that feature results in a large change in the predicted label.\n",
    "\n",
    "### 2. Model-Agnostic Techniques\n",
    "\n",
    "These methods treat the trained model as a black box and can be applied universally to determine feature impact, even for complex models like Neural Networks or Support Vector Machines (SVMs).\n",
    "\n",
    "* **Permutation Feature Importance (PFI):** This technique measures how much the model's performance decreases when the values of a single feature are randomly shuffled (or permuted).\n",
    "    * **Impact:** If shuffling a feature causes a large drop in accuracy, that feature is highly important. If shuffling it causes little change, it's not very important. PFI is a robust way to gauge the sensitivity of the label to that feature.\n",
    "* **SHAP (SHapley Additive exPlanations):** Based on game theory, SHAP provides a unified measure of feature importance by calculating how much each feature contributes to pushing the model's output from the average prediction toward the current prediction.\n",
    "    * **Benefit:** SHAP can explain *global* feature impact (average impact across all data) and *local* impact (why a single prediction was made).\n",
    "* **LIME (Local Interpretable Model-agnostic Explanations):** LIME aims to explain individual predictions. It works by slightly perturbing the data around a specific instance and training a simple, interpretable model (like a linear model) on the generated data to see which features were most influential for that specific prediction.\n",
    "\n",
    "By using these techniques, you can confidently identify which input features your trained model is most sensitive to and which ones have the biggest impact on the final label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e99ef8-b0d5-4374-9326-3f3b1ed895a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
