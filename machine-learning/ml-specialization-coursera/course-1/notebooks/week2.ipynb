{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80774fd4-feb8-4535-a591-af9ac337df64",
   "metadata": {},
   "source": [
    "# Week 2: Regression with multiple input variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2eaca2-08e6-47d3-85e9-d3bf87d6077e",
   "metadata": {},
   "source": [
    "## Multiple features\n",
    "\n",
    "When we have more than one feature, we can rewrite the our linear regression model as below:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "f_{w,b}(\\mathbf{x}) &= w_1x_1 + w_2x_2 + ... + w_nx_n + b \\\\\n",
    "f_{w,b}(\\mathbf{x}) &= \\mathbf{\\vec w}.\\mathbf{\\vec x} + b = \\mathbf{w}^T\\mathbf{x} + b\n",
    "\\end{aligned}\n",
    "\\label{eq: multiple_feat}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In above eqaution, $\\mathbf{x}$ and $\\mathbf{w}$ are $n\\times 1$ vector of features and weights, respectively, and $b$ is a real number. We call this model **multiple linear regression** and NOT multivariate regression (this is something different).\n",
    "\n",
    "In Python, we can use `Numpy`'s vectorized function, `numpy.dot()` to calculate $\\mathbf{\\vec w}.\\mathbf{\\vec x}$:\n",
    "\n",
    "```Python\n",
    "import numpy as np\n",
    "f = np.dot(w, x) + b\n",
    "```\n",
    "\n",
    "## Vectorization\n",
    "\n",
    "Here is a high-level comparison of vectorization and non-vectorizarion under-the-hood implementation:\n",
    "\n",
    "![vectorization](../images/vectorization.png)\n",
    "\n",
    "## Broadcasting in Python\n",
    "Broadcasting in Python refers to a feature in libraries like **NumPy** that allows arithmetic operations between arrays of different shapes without creating duplicate copies of the data. Instead of requiring arrays to have identical dimensions for element-wise operations, broadcasting automatically \"stretches\" the smaller array to match the shape of the larger one, making operations more efficient in terms of both memory and speed.\n",
    "\n",
    "### How Broadcasting Works\n",
    "The core principle of broadcasting is a set of rules that determine if two arrays are \"broadcastable.\" Two arrays are compatible for broadcasting if, for each dimension, they are either:\n",
    "* Equal in size.\n",
    "* One of the dimensions is 1.\n",
    "\n",
    "The process aligns the arrays by adding new dimensions with size 1 to the smaller array on its left side until the arrays have the same number of dimensions. Then, it checks the dimensions one by one, from the last dimension to the first. \n",
    "\n",
    "Let's illustrate with an example: a $2 \\times 3$ array and a $1 \\times 3$ array.\n",
    "* **Array A:** `[[1, 2, 3], [4, 5, 6]]` (Shape: $2 \\times 3$)\n",
    "* **Array B:** `[10, 20, 30]` (Shape: $3$)\n",
    "\n",
    "To perform an element-wise operation, NumPy adds a new dimension to Array B, making its shape $1 \\times 3$. It then \"broadcasts\" the values of Array B across the rows of Array A.\n",
    "\n",
    "The operation then becomes:\n",
    "$$\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} + \\begin{bmatrix} 10 & 20 & 30 \\\\ 10 & 20 & 30 \\end{bmatrix} = \\begin{bmatrix} 11 & 22 & 33 \\\\ 14 & 25 & 36 \\end{bmatrix}$$\n",
    "\n",
    "### Why Use Broadcasting?\n",
    "Broadcasting offers significant advantages, especially in scientific computing and data analysis.\n",
    "\n",
    "**Efficiency**\n",
    "\n",
    "It avoids the need to create explicit large temporary arrays, which saves **memory** and is faster. For large datasets, this can be the difference between an operation that runs in seconds and one that crashes due to insufficient memory.\n",
    "\n",
    "**Code Simplicity**\n",
    "\n",
    "Broadcasting makes code cleaner and more readable. Instead of writing loops to apply an operation to each element, you can use a single, concise line of code. For example, to add a vector to each row of a matrix, you can simply write `matrix + vector` instead of iterating through each row.\n",
    "\n",
    "### Performance\n",
    "Because broadcasting is implemented in optimized, low-level C code within NumPy, it's often much faster than an equivalent operation written with Python loops.\n",
    "\n",
    "### Limitations\n",
    "While powerful, broadcasting has its own set of rules and limitations. If the arrays don't satisfy the broadcasting rules, NumPy will raise a `ValueError`. For example, trying to add a $2 \\times 3$ array to a $1 \\times 4$ array will fail because the last dimensions (3 and 4) are not equal or one of them is not 1. In such cases, you must reshape the arrays to make them compatible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15532e43-75a2-4b0c-bb40-119b1577f604",
   "metadata": {},
   "source": [
    "### Gradient descent for multiple linear regression\n",
    "\n",
    "![multifeat multiple linear regression](../images/multifeat_lreg_gd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7548718b-325f-4d6c-90c5-dd8e67e61d4c",
   "metadata": {},
   "source": [
    "### Feature scaling\n",
    "\n",
    "Here is an intuition of how twicking different features range into comparable scales would increase gradient descent converage rate:\n",
    "\n",
    "![feature scaling](../images/feat_scaling.png)\n",
    "\n",
    "Typically, there are two common ways to scale features:\n",
    "- Normalizing to feature range: divide data by range ($\\frac{x - x_{min}}{x_{max} - x_{min}}$)\n",
    "- Mean normalization: subtract data from mean and divide the result to data range ($\\frac{x-\\mu}{x_{max} - x_{min}}$)\n",
    "- Z-score (standard) normalization: subtract data from mean and divide the result by standard deviation ($\\frac{x-\\mu}{\\sigma}$)\n",
    "\n",
    "\n",
    "**Rule of thumb for feature scaling:**\n",
    "\n",
    "![feature scaling rule](../images/feat_scaling_rule.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6631f37-2ad7-40ad-8b71-9ff9ee0d5c3a",
   "metadata": {},
   "source": [
    "### Choosing learning rate\n",
    "\n",
    "![learning rate](../images/learning_rate.png)\n",
    "\n",
    "\n",
    "**Implementation Note:** When normalizing the features, it is important to store the values used for normalization - the mean value and the standard deviation used for the computations. After learning the parameters from the model, we often want to predict the prices of houses we have not seen before. Given a new x value (living room area and number of bed- rooms), we must <u>first normalize x using the mean and standard deviation that we had previously computed from the training set</u>.\n",
    "\n",
    "**Plotting data:**\n",
    "- With multiple features, we can no longer have a single plot showing results versus features.\n",
    "- When generating the plot, the normalized features were used. Any predictions using the parameters learned from a normalized training set must also be normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9746dbdc-0222-42dc-991a-3694221ed92e",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Using intuition to design bew features, by transforming or combining original features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8b6c7a-6c83-47d6-9859-87772c23e9a6",
   "metadata": {},
   "source": [
    "### Polynomial regression\n",
    "\n",
    "![polynomial regression](../images/polynomial_reg.png)\n",
    "\n",
    "Feature scaling is very important when we using higher orders of features because scales of features should be comparable to be able to make gradient descent work.\n",
    "\n",
    "We need to make a choice what feature we should use or engineer to get e better model (will see in Course 2).\n",
    "\n",
    "**Importance of certain features:**\n",
    "- Gradient descent is picking the 'correct' features for us by emphasizing its associated parameter.\n",
    "- Less weight value implies less important/correct feature, and in extreme, when the weight becomes zero or very close to zero, the associated feature is not useful in fitting the model to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e3acd-d586-4777-a338-e7e06fb4d5f3",
   "metadata": {},
   "source": [
    "**A note on Numpy (numpy.c_):**\n",
    "\n",
    "- `numpy.c_` is a convenient way to concatenate arrays along the second axis (column-wise).\n",
    "- It's a special object in NumPy that acts as a shorthand for column stacking and is often more readable than using `np.concatenate` or `np.stack`.\n",
    "- The `c_` object takes a sequence of array-like objects and stacks them as columns in a 2D array. The input arrays are automatically converted into 2D if they are 1D.\n",
    "- Note that `np.c_` is not a function; it's an instance of a class (`numpy.lib.index_tricks.CClass`). When you use bracket notation (like `[]`), you're not calling a function but rather invoking the `__getitem__` method of this class. For example, `np.c_[a, b, c]` is a more compact way of writing:\n",
    "\n",
    "\n",
    "```Python\n",
    "np.concatenate([a[..., np.newaxis], b[..., np.newaxis], c[..., np.newaxis]], axis=1)\n",
    "```\n",
    "\n",
    "**Difference between `@` operator and numpy.dot() function:**\n",
    "\n",
    "The primary difference is that `@` is an **operator** for matrix multiplication, while `numpy.dot()` is a **function**. While they often produce the same result for 2D arrays, they behave differently in other cases and have distinct purposes.\n",
    "\n",
    "#### `@` Operator (Python 3.5+)\n",
    "\n",
    "* `@` is a dedicated **matrix multiplication operator**. It was introduced in Python 3.5 to provide a more intuitive and readable syntax for this operation.\n",
    "* **Behavior:** It performs matrix multiplication on 2D arrays. For 1D arrays, it performs the dot product. However, it's designed to be used with objects that support matrix multiplication, such as NumPy arrays.\n",
    "* **Broadcasting:** The `@` operator does **not** support broadcasting in the same way as other NumPy operators like `*` or `+`. Both arrays must be of a compatible shape for matrix multiplication (i.e., the number of columns of the first array must match the number of rows of the second). \n",
    "\n",
    "#### `numpy.dot()` function\n",
    "\n",
    "* `numpy.dot()` is a more general-purpose **function** that calculates the dot product of two arrays.\n",
    "* **Behavior:**\n",
    "    * For 2D arrays, `np.dot(a, b)` performs matrix multiplication.\n",
    "    * For 1D arrays, it computes the inner product (the sum of the products of corresponding elements).\n",
    "    * For a scalar and an array, it performs element-wise multiplication.\n",
    "    * It can also handle higher-dimensional arrays.\n",
    "* **Broadcasting:** Like the `@` operator, `np.dot()` does not have the same flexible broadcasting as element-wise operations.\n",
    "\n",
    "#### Practical Differences and Recommendations\n",
    "\n",
    "| Feature | `@` Operator | `numpy.dot()` Function |\n",
    "| :--- | :--- | :--- |\n",
    "| **Readability** | High: Clear intention for matrix multiplication. | Lower: Can be ambiguous depending on context. |\n",
    "| **Flexibility** | Less flexible: Primarily for matrix multiplication. | More flexible: Can do inner product, element-wise, and matrix multiplication. |\n",
    "| **Speed** | Often faster, as it's directly implemented for matrix multiplication. | Slightly slower in some cases, as it has more general logic to handle various dimensions. |\n",
    "\n",
    "**Recommendation:**\n",
    "* Use the **`@` operator** for **matrix multiplication of 2D arrays**. It's the most readable and modern approach for this specific task.\n",
    "* Use **`numpy.dot()`** for **dot products of 1D vectors** or when you need its more general behavior for higher-dimensional arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bafc34-3bb1-4b44-b1d1-ed3d41e5938f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
