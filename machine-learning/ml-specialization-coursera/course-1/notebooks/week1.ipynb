{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "933dbd0a-6659-4bbe-ba57-d03d16c37458",
   "metadata": {},
   "source": [
    "# Week 1: Introduction to Machine Learning\n",
    "\n",
    "## Supervised learning\n",
    "\n",
    "**Regression:** Predicting a number out of infinitely many posssible outputs\n",
    "\n",
    "**Classification:** Predicting categories out of small number of possible outputs\n",
    "\n",
    "## Unsupervised learning\n",
    "\n",
    "Find something interesting (*e.g.* patterns) in <u>unlabeled</u> data.\n",
    "\n",
    "Three common algorithms in unsupervised learning:\n",
    "- **Clustering:** group similar data together (e.g. Similar news, similar customers based on their shopping habits, similar mutants based on their gene expression profiles (microarray data)\n",
    "- **Anomaly detection:** find unusual data points: (e.g. outliers)\n",
    "- **Dimentionality reduction:** compress data using fewer numbers (e.g. data compression)\n",
    "\n",
    "## Linear regression\n",
    "\n",
    "**Notation:**\n",
    "\n",
    "Here is a summary of some of the notation you will encounter.  \n",
    "\n",
    "|Notation |Description|Python (if applicable)|\n",
    "|---------|-----------|----------------------|\n",
    "| $a$  | scalar, non bold ||\n",
    "| $\\mathbf{a}$ | vector, bold||\n",
    "| **Regression** |  |  |\n",
    "|  $\\mathbf{x}$ | Training Example feature values (in this lab - Size (1000 sqft))  | `x_train` |   \n",
    "|  $\\mathbf{y}$  | Training Example  targets (in this lab Price (1000s of dollars))  | `y_train` \n",
    "|  $x^{(i)}$, $y^{(i)}$ | $i_{th}$Training Example | `x_i`, `y_i`|\n",
    "| m | Number of training examples | `m`|\n",
    "|  $w$  |  parameter: weight                                 | `w`    |\n",
    "|  $b$           |  parameter: bias                                           | `b`    |     \n",
    "| $f_{w,b}(x^{(i)})$ | The result of the model evaluation at $x^{(i)}$ parameterized by $w,b$: $f_{w,b}(x^{(i)}) = wx^{(i)}+b$  | `f_wb` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d5e718-686f-4111-b013-cfb07633c94d",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "\n",
    "Cost functions are defined to assess model's performance. One of the most common cost functions that is used is Mean Squared Error (MSE):\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{2m}\\sum_{i=1}^m (\\hat y^{(i)} - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "where $\\hat y{(i)} = f_{w,b}(x^{(i)}) = wx^{(i)} + b$ and $2$ in the denominator is usually added to the denominator to make the subsequent calculations neater and it won't affect our cost analyses. The goal here is find a $w$ and a $b$ to minimize $J(w, b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5dee41-ad66-47c6-938e-4ac0690e8bda",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Gradient descent is a powerful iterative method to calculate an extermum of a function.\n",
    "\n",
    "When it comes to cost functions, the goal is to find the values for the model parameters ($w$ and $b$) that result in the lowest possible cost. This process is known as minimizing the cost function. Gradient descent is an algorithm for finding values of parameters $w$ and $b$ that minimize the cost function $J(w,b)$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w &= w - \\alpha \\frac{\\partial}{\\partial w}J(w,b) \\\\\n",
    "b &= b - \\alpha \\frac{\\partial}{\\partial b}J(w,b)\n",
    "\\end{aligned}\n",
    "\\label{eq: gradient_descent}\n",
    "$$\n",
    "\n",
    "\n",
    "where $\\alpha$ is learning rate in Equation {eq}`eq: gradient_descent`  In this algorithm, both parameters must be updated simultaneously until convergence is acheived.\n",
    "\n",
    "Picking an appropriate learning rate is key for the success of the algorithm:\n",
    "- **Too small:** Very low convergence rate\n",
    "- **Too large:** Never converges (overshoot)\n",
    "\n",
    "Now, let's formulate gradient descent for minimizing MSE cost function for a linear regression model:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial w} J(w,b) &= \\frac{1}{m} \\sum_{i=1}^m (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\\\\n",
    "\\frac{\\partial}{\\partial b} J(w,b) &= \\frac{1}{m} \\sum_{i=1}^m (f_{w,b}(x^{(i)}) - y^{(i)})\n",
    "\\end{aligned}\n",
    "\\label{eq: gd_lreg}\n",
    "$$\n",
    "\n",
    "In MSE cost function is a convex function, meaning that it has only one minimum. So, we won't get trapped in local minima when applying the algorithm. Therefore, if we picked an appropriate learning rate, we are gauranteed to converge to the minimum.\n",
    "\n",
    "**\"Batch\"** gradient descent: Each step of gradient descent uses all the training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab41bb2a-e17f-4bbd-919c-c11f61b11199",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
